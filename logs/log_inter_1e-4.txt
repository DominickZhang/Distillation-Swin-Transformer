[2021-09-22 19:50:43 swin_tiny_patch4_window7_224] (main.py 718): INFO Full config saved to output/swin_tiny_patch4_window7_224/test_inter_all/config.json
[2021-09-22 19:50:43 swin_tiny_patch4_window7_224] (main.py 721): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /root/FastBaseline/data/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
DISTILL:
  ACCUMULATE_STEPS: 0
  ALPHA: 0.0
  DO_DISTILL: true
  INTERMEDIATE_CHECKPOINT: ''
  LOAD_TAR: false
  STAGE: -1
  TEACHER: /mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
  TEMPERATURE: 1.0
  TRAIN_INTERMEDIATE: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin_distill
OUTPUT: output/swin_tiny_patch4_window7_224/test_inter_all
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: test_inter_all
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EPOCHS: 2
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2021-09-22 19:50:49 swin_tiny_patch4_window7_224] (main.py 131): INFO Loading teacher model:swin_distill//mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
[2021-09-22 19:50:53 swin_tiny_patch4_window7_224] (main.py 137): INFO <All keys matched successfully>
[2021-09-22 19:50:53 swin_tiny_patch4_window7_224] (main.py 141): INFO Creating model:swin_distill/swin_tiny_patch4_window7_224
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 144): INFO SwinTransformerDistill(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayerDistill(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayerDistill(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayerDistill(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayerDistill(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
  (fit_dense_C): ModuleList(
    (0): Linear(in_features=96, out_features=192, bias=True)
    (1): Linear(in_features=192, out_features=384, bias=True)
    (2): Linear(in_features=384, out_features=768, bias=True)
    (3): Linear(in_features=768, out_features=1536, bias=True)
  )
  (fit_dense_M): ModuleList(
    (0): Linear(in_features=3, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=12, bias=True)
    (2): Linear(in_features=12, out_features=24, bias=True)
    (3): Linear(in_features=24, out_features=48, bias=True)
  )
)
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 154): INFO number of params: 29859574
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 157): INFO number of GFLOPs: 4.49440512
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 186): INFO no checkpoint found in output/swin_tiny_patch4_window7_224/test_inter_all, ignoring auto resume
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 203): INFO Start training
[2021-09-22 19:50:54 swin_tiny_patch4_window7_224] (main.py 268): INFO Training stage: -1...
[2021-09-22 19:51:05 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [0/2][0/1251]	eta 3:48:25 lr 0.000100	time 10.9555 (10.9555)	loss 1425.1339 (1425.1339)	attn_loss 1406.6858 (1406.6858)	hidden_loss 18.4482 (18.4482)	grad_norm inf (inf)	mem 23404MB
[2021-09-22 19:51:16 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [0/2][10/1251]	eta 0:42:33 lr 0.000100	time 1.1414 (2.0576)	loss 1423.1675 (1426.6692)	attn_loss 1402.7085 (1406.8328)	hidden_loss 20.4590 (19.8364)	grad_norm 1561.6703 (inf)	mem 24579MB
[2021-09-22 19:51:18 swin_tiny_patch4_window7_224] (main.py 369): INFO EPOCH 0 training takes 0:00:23
[2021-09-22 19:51:18 swin_tiny_patch4_window7_224] (utils.py 63): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth saving......
[2021-09-22 19:51:19 swin_tiny_patch4_window7_224] (utils.py 65): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth saved !!!
[2021-09-22 19:51:27 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [1/2][0/1251]	eta 2:49:32 lr 0.000100	time 8.1316 (8.1316)	loss 1417.5261 (1417.5261)	attn_loss 1397.4824 (1397.4824)	hidden_loss 20.0438 (20.0438)	grad_norm 1559.5461 (1559.5461)	mem 24579MB
[2021-09-22 19:52:02 swin_tiny_patch4_window7_224] (main.py 720): INFO Full config saved to output/swin_tiny_patch4_window7_224/test_inter_all/config.json
[2021-09-22 19:52:02 swin_tiny_patch4_window7_224] (main.py 723): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /root/FastBaseline/data/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
DISTILL:
  ACCUMULATE_STEPS: 0
  ALPHA: 0.0
  DO_DISTILL: true
  INTERMEDIATE_CHECKPOINT: ''
  LOAD_TAR: false
  STAGE: -1
  TEACHER: /mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
  TEMPERATURE: 1.0
  TRAIN_INTERMEDIATE: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin_distill
OUTPUT: output/swin_tiny_patch4_window7_224/test_inter_all
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: test_inter_all
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EPOCHS: 3
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2021-09-22 19:52:06 swin_tiny_patch4_window7_224] (main.py 131): INFO Loading teacher model:swin_distill//mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
[2021-09-22 19:52:11 swin_tiny_patch4_window7_224] (main.py 137): INFO <All keys matched successfully>
[2021-09-22 19:52:11 swin_tiny_patch4_window7_224] (main.py 141): INFO Creating model:swin_distill/swin_tiny_patch4_window7_224
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (main.py 144): INFO SwinTransformerDistill(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayerDistill(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayerDistill(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayerDistill(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayerDistill(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
  (fit_dense_C): ModuleList(
    (0): Linear(in_features=96, out_features=192, bias=True)
    (1): Linear(in_features=192, out_features=384, bias=True)
    (2): Linear(in_features=384, out_features=768, bias=True)
    (3): Linear(in_features=768, out_features=1536, bias=True)
  )
  (fit_dense_M): ModuleList(
    (0): Linear(in_features=3, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=12, bias=True)
    (2): Linear(in_features=12, out_features=24, bias=True)
    (3): Linear(in_features=24, out_features=48, bias=True)
  )
)
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (main.py 154): INFO number of params: 29859574
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (main.py 157): INFO number of GFLOPs: 4.49440512
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (main.py 184): INFO auto resuming from output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth....................
[2021-09-22 19:52:12 swin_tiny_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-09-22 19:52:21 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [0/49]	Time 9.618 (9.618)	Attention_Loss_0 301.3568 (301.3568)	Attention_Loss_1 456.3572 (456.3572)	Attention_Loss_2 882.9709 (882.9709)	Attention_Loss_3 21.4544 (21.4544)	Hidden_Loss_0 1.8449 (1.8449)	Hidden_Loss_1 3.9089 (3.9089)	Hidden_Loss_2 2.4278 (2.4278)	Hidden_Loss_3 112.7924 (112.7924)	Mem 9282MB
[2021-09-22 19:52:37 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [10/49]	Time 1.598 (2.337)	Attention_Loss_0 302.8281 (301.4261)	Attention_Loss_1 472.3116 (460.3557)	Attention_Loss_2 892.5585 (887.9023)	Attention_Loss_3 21.3095 (21.4866)	Hidden_Loss_0 1.8462 (1.8487)	Hidden_Loss_1 3.8949 (3.9079)	Hidden_Loss_2 2.4116 (2.4172)	Hidden_Loss_3 118.4029 (113.0278)	Mem 9731MB
[2021-09-22 19:52:54 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [20/49]	Time 1.600 (1.988)	Attention_Loss_0 302.5440 (301.3319)	Attention_Loss_1 461.4515 (461.1334)	Attention_Loss_2 888.5661 (888.2874)	Attention_Loss_3 21.4568 (21.5058)	Hidden_Loss_0 1.8675 (1.8437)	Hidden_Loss_1 3.9220 (3.9124)	Hidden_Loss_2 2.4243 (2.4208)	Hidden_Loss_3 114.0087 (112.8711)	Mem 9731MB
[2021-09-22 19:53:10 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [30/49]	Time 1.600 (1.865)	Attention_Loss_0 304.2793 (301.4996)	Attention_Loss_1 460.8892 (460.4704)	Attention_Loss_2 891.4523 (888.3864)	Attention_Loss_3 21.5649 (21.5065)	Hidden_Loss_0 1.8180 (1.8438)	Hidden_Loss_1 3.9007 (3.9110)	Hidden_Loss_2 2.4332 (2.4229)	Hidden_Loss_3 111.8679 (113.0182)	Mem 9731MB
[2021-09-22 19:53:26 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [40/49]	Time 1.601 (1.801)	Attention_Loss_0 308.7317 (301.4403)	Attention_Loss_1 461.6493 (460.4553)	Attention_Loss_2 885.5984 (888.4752)	Attention_Loss_3 21.5446 (21.5184)	Hidden_Loss_0 1.8024 (1.8424)	Hidden_Loss_1 3.9191 (3.9119)	Hidden_Loss_2 2.4268 (2.4245)	Hidden_Loss_3 112.0250 (112.8307)	Mem 9731MB
[2021-09-22 19:53:38 swin_tiny_patch4_window7_224] (main.py 632): INFO  * Attention_Loss_0 301.572 Attention_Loss_1 460.834 Attention_Loss_2 888.457 Attention_Loss_3 21.521 Hidden_Loss_0 1.843 Hidden_Loss_1 3.911 Hidden_Loss_2 2.424 Hidden_Loss_3 113.069
[2021-09-22 19:53:38 swin_tiny_patch4_window7_224] (main.py 203): INFO Start training
[2021-09-22 19:53:38 swin_tiny_patch4_window7_224] (main.py 268): INFO Training stage: -1...
[2021-09-22 19:53:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][0/1251]	eta 3:07:28 lr 0.000100	time 8.9918 (8.9918)	loss 601.6999 (601.6999)	attn_loss 584.5553 (584.5553)	hidden_loss 17.1446 (17.1446)	grad_norm inf (inf)	mem 26097MB
[2021-09-22 19:54:00 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][10/1251]	eta 0:40:03 lr 0.000100	time 1.1816 (1.9367)	loss 571.4211 (589.9891)	attn_loss 552.6071 (571.5507)	hidden_loss 18.8140 (18.4384)	grad_norm 661.5436 (inf)	mem 26441MB
[2021-09-22 19:54:12 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][20/1251]	eta 0:32:30 lr 0.000100	time 1.2099 (1.5847)	loss 537.9066 (577.1704)	attn_loss 524.8994 (560.2727)	hidden_loss 13.0072 (16.8977)	grad_norm 638.3439 (inf)	mem 26444MB
[2021-09-22 19:54:24 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][30/1251]	eta 0:29:42 lr 0.000100	time 1.1849 (1.4596)	loss 558.7511 (567.9369)	attn_loss 548.0214 (552.9482)	hidden_loss 10.7297 (14.9887)	grad_norm 629.5686 (inf)	mem 26445MB
[2021-09-22 19:54:36 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][40/1251]	eta 0:28:09 lr 0.000100	time 1.1887 (1.3954)	loss 521.2802 (560.5155)	attn_loss 510.9482 (546.6214)	hidden_loss 10.3320 (13.8941)	grad_norm 612.6534 (inf)	mem 26445MB
[2021-09-22 19:54:48 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][50/1251]	eta 0:27:10 lr 0.000100	time 1.2303 (1.3574)	loss 527.5011 (553.3489)	attn_loss 518.0105 (540.2165)	hidden_loss 9.4906 (13.1325)	grad_norm 603.1002 (inf)	mem 26448MB
[2021-09-22 19:55:00 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][60/1251]	eta 0:26:25 lr 0.000100	time 1.2144 (1.3312)	loss 518.8321 (547.4005)	attn_loss 508.7086 (534.8124)	hidden_loss 10.1236 (12.5882)	grad_norm 618.5997 (inf)	mem 26448MB
[2021-09-22 19:55:12 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][70/1251]	eta 0:25:49 lr 0.000100	time 1.2125 (1.3124)	loss 473.5629 (539.8681)	attn_loss 463.6567 (527.6893)	hidden_loss 9.9062 (12.1788)	grad_norm 627.4661 (inf)	mem 26448MB
[2021-09-22 19:55:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][80/1251]	eta 0:25:19 lr 0.000100	time 1.1941 (1.2979)	loss 464.9128 (531.6628)	attn_loss 455.0516 (519.7949)	hidden_loss 9.8611 (11.8679)	grad_norm 641.2421 (inf)	mem 26448MB
[2021-09-22 19:55:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][90/1251]	eta 0:24:54 lr 0.000100	time 1.1893 (1.2871)	loss 473.9941 (524.1235)	attn_loss 465.0241 (512.5120)	hidden_loss 8.9700 (11.6115)	grad_norm 624.9740 (inf)	mem 26448MB
[2021-09-22 19:55:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][100/1251]	eta 0:24:31 lr 0.000100	time 1.2103 (1.2785)	loss 457.1757 (517.1477)	attn_loss 448.0322 (505.7566)	hidden_loss 9.1434 (11.3912)	grad_norm 589.5659 (inf)	mem 26448MB
[2021-09-22 19:55:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][110/1251]	eta 0:24:09 lr 0.000100	time 1.1850 (1.2704)	loss 448.6848 (510.9799)	attn_loss 438.9475 (499.7720)	hidden_loss 9.7373 (11.2079)	grad_norm 561.6517 (inf)	mem 26448MB
[2021-09-22 19:56:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][120/1251]	eta 0:23:50 lr 0.000100	time 1.1918 (1.2644)	loss 434.5061 (504.5455)	attn_loss 425.0050 (493.4947)	hidden_loss 9.5011 (11.0508)	grad_norm 517.9891 (inf)	mem 26448MB
[2021-09-22 19:56:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][130/1251]	eta 0:23:31 lr 0.000100	time 1.2046 (1.2594)	loss 401.6044 (498.4672)	attn_loss 392.2623 (487.5534)	hidden_loss 9.3421 (10.9138)	grad_norm 455.6089 (inf)	mem 26450MB
[2021-09-22 19:56:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][140/1251]	eta 0:23:14 lr 0.000100	time 1.1943 (1.2549)	loss 424.7162 (492.8336)	attn_loss 415.7606 (482.0400)	hidden_loss 8.9557 (10.7936)	grad_norm 423.5566 (inf)	mem 26450MB
[2021-09-22 19:56:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][150/1251]	eta 0:22:57 lr 0.000100	time 1.2196 (1.2513)	loss 410.0054 (487.2774)	attn_loss 400.4319 (476.5814)	hidden_loss 9.5735 (10.6960)	grad_norm 362.4929 (inf)	mem 26450MB
[2021-09-22 19:56:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][160/1251]	eta 0:22:41 lr 0.000100	time 1.2012 (1.2480)	loss 384.4327 (482.1096)	attn_loss 375.0056 (471.5125)	hidden_loss 9.4271 (10.5970)	grad_norm 325.6632 (inf)	mem 26450MB
[2021-09-22 19:57:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][170/1251]	eta 0:22:26 lr 0.000100	time 1.2138 (1.2451)	loss 388.0398 (476.9879)	attn_loss 378.6088 (466.4665)	hidden_loss 9.4310 (10.5214)	grad_norm 330.1830 (inf)	mem 26450MB
[2021-09-22 19:57:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][180/1251]	eta 0:22:11 lr 0.000100	time 1.2069 (1.2428)	loss 373.3370 (472.5344)	attn_loss 364.0283 (462.0848)	hidden_loss 9.3087 (10.4496)	grad_norm 293.6785 (inf)	mem 26450MB
[2021-09-22 19:57:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][190/1251]	eta 0:21:56 lr 0.000100	time 1.1873 (1.2405)	loss 391.9772 (468.3780)	attn_loss 382.4747 (457.9940)	hidden_loss 9.5024 (10.3840)	grad_norm 272.3214 (inf)	mem 26450MB
[2021-09-22 19:57:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][200/1251]	eta 0:21:41 lr 0.000100	time 1.1932 (1.2386)	loss 378.3817 (464.4181)	attn_loss 368.9473 (454.0901)	hidden_loss 9.4344 (10.3279)	grad_norm 277.4998 (inf)	mem 26450MB
[2021-09-22 19:57:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][210/1251]	eta 0:21:27 lr 0.000100	time 1.1863 (1.2367)	loss 365.9088 (460.4381)	attn_loss 356.5912 (450.1623)	hidden_loss 9.3176 (10.2759)	grad_norm 259.0245 (inf)	mem 26450MB
[2021-09-22 19:58:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][220/1251]	eta 0:21:13 lr 0.000100	time 1.2028 (1.2350)	loss 383.3028 (456.6999)	attn_loss 374.3528 (446.4752)	hidden_loss 8.9500 (10.2246)	grad_norm 241.1180 (inf)	mem 26450MB
[2021-09-22 19:58:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][230/1251]	eta 0:20:59 lr 0.000100	time 1.2136 (1.2334)	loss 372.3258 (452.9901)	attn_loss 363.0361 (442.8166)	hidden_loss 9.2897 (10.1734)	grad_norm 228.2698 (inf)	mem 26450MB
[2021-09-22 19:58:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][240/1251]	eta 0:20:45 lr 0.000100	time 1.1879 (1.2320)	loss 372.8568 (449.4406)	attn_loss 364.0521 (439.3175)	hidden_loss 8.8048 (10.1231)	grad_norm 223.6452 (inf)	mem 26450MB
[2021-09-22 19:58:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][250/1251]	eta 0:20:31 lr 0.000100	time 1.1946 (1.2307)	loss 366.8630 (445.9195)	attn_loss 358.2046 (435.8430)	hidden_loss 8.6585 (10.0765)	grad_norm 193.6784 (inf)	mem 26450MB
[2021-09-22 19:58:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][260/1251]	eta 0:20:18 lr 0.000100	time 1.2044 (1.2296)	loss 362.0131 (442.5122)	attn_loss 353.3983 (432.4870)	hidden_loss 8.6148 (10.0251)	grad_norm 182.2042 (inf)	mem 26450MB
[2021-09-22 19:59:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][270/1251]	eta 0:20:05 lr 0.000100	time 1.1891 (1.2284)	loss 360.7370 (439.3915)	attn_loss 352.6049 (429.4228)	hidden_loss 8.1322 (9.9686)	grad_norm 168.4985 (inf)	mem 26450MB
[2021-09-22 19:59:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][280/1251]	eta 0:19:51 lr 0.000100	time 1.2221 (1.2274)	loss 340.2923 (436.0662)	attn_loss 331.7422 (426.1536)	hidden_loss 8.5501 (9.9127)	grad_norm 170.5903 (inf)	mem 26450MB
[2021-09-22 19:59:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][290/1251]	eta 0:19:38 lr 0.000100	time 1.1975 (1.2264)	loss 348.5962 (433.0691)	attn_loss 340.2681 (423.2106)	hidden_loss 8.3282 (9.8585)	grad_norm 159.1188 (inf)	mem 26450MB
[2021-09-22 19:59:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][300/1251]	eta 0:19:25 lr 0.000100	time 1.1860 (1.2256)	loss 346.2787 (430.0994)	attn_loss 338.0181 (420.2907)	hidden_loss 8.2606 (9.8087)	grad_norm 159.6004 (inf)	mem 26450MB
[2021-09-22 19:59:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][310/1251]	eta 0:19:12 lr 0.000100	time 1.1946 (1.2247)	loss 323.8191 (427.1698)	attn_loss 315.6744 (417.4136)	hidden_loss 8.1446 (9.7562)	grad_norm 146.2475 (inf)	mem 26450MB
[2021-09-22 20:00:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][320/1251]	eta 0:18:59 lr 0.000100	time 1.1893 (1.2239)	loss 344.9607 (424.3228)	attn_loss 337.7524 (414.6268)	hidden_loss 7.2083 (9.6961)	grad_norm 128.4081 (inf)	mem 26450MB
[2021-09-22 20:00:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][330/1251]	eta 0:18:46 lr 0.000100	time 1.2272 (1.2233)	loss 334.7316 (421.6568)	attn_loss 326.6595 (412.0187)	hidden_loss 8.0721 (9.6381)	grad_norm 131.5727 (inf)	mem 26450MB
[2021-09-22 20:00:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][340/1251]	eta 0:18:33 lr 0.000100	time 1.1880 (1.2225)	loss 319.7385 (418.9314)	attn_loss 312.0160 (409.3461)	hidden_loss 7.7225 (9.5852)	grad_norm 117.1605 (inf)	mem 26450MB
[2021-09-22 20:00:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][350/1251]	eta 0:18:21 lr 0.000100	time 1.2064 (1.2220)	loss 337.4317 (416.4874)	attn_loss 330.0141 (406.9584)	hidden_loss 7.4176 (9.5290)	grad_norm 109.7573 (inf)	mem 26450MB
[2021-09-22 20:00:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][360/1251]	eta 0:18:08 lr 0.000100	time 1.1950 (1.2215)	loss 334.2776 (414.1334)	attn_loss 326.6123 (404.6554)	hidden_loss 7.6653 (9.4780)	grad_norm 115.8703 (inf)	mem 26450MB
[2021-09-22 20:01:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][370/1251]	eta 0:17:55 lr 0.000100	time 1.2193 (1.2209)	loss 326.5579 (411.7943)	attn_loss 317.9603 (402.3624)	hidden_loss 8.5975 (9.4319)	grad_norm 97.5202 (inf)	mem 26450MB
[2021-09-22 20:01:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][380/1251]	eta 0:17:42 lr 0.000100	time 1.2065 (1.2204)	loss 325.6114 (409.5522)	attn_loss 317.5604 (400.1660)	hidden_loss 8.0510 (9.3862)	grad_norm 92.7272 (inf)	mem 26450MB
[2021-09-22 20:01:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][390/1251]	eta 0:17:30 lr 0.000100	time 1.1961 (1.2198)	loss 333.2593 (407.4602)	attn_loss 326.0872 (398.1225)	hidden_loss 7.1722 (9.3378)	grad_norm 92.5051 (inf)	mem 26450MB
[2021-09-22 20:01:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][400/1251]	eta 0:17:17 lr 0.000100	time 1.1964 (1.2193)	loss 313.0148 (405.3745)	attn_loss 305.8442 (396.0865)	hidden_loss 7.1706 (9.2880)	grad_norm 84.8734 (inf)	mem 26450MB
[2021-09-22 20:01:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][410/1251]	eta 0:17:05 lr 0.000100	time 1.2042 (1.2188)	loss 315.9269 (403.3806)	attn_loss 308.4703 (394.1374)	hidden_loss 7.4566 (9.2433)	grad_norm 81.7637 (inf)	mem 26450MB
[2021-09-22 20:02:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][420/1251]	eta 0:16:52 lr 0.000100	time 1.2002 (1.2184)	loss 328.0493 (401.4622)	attn_loss 321.0989 (392.2625)	hidden_loss 6.9504 (9.1997)	grad_norm 89.7915 (inf)	mem 26450MB
[2021-09-22 20:02:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][430/1251]	eta 0:16:39 lr 0.000100	time 1.1977 (1.2179)	loss 324.7883 (399.6044)	attn_loss 317.6866 (390.4485)	hidden_loss 7.1017 (9.1559)	grad_norm 78.4921 (inf)	mem 26450MB
[2021-09-22 20:02:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][440/1251]	eta 0:16:27 lr 0.000100	time 1.1903 (1.2174)	loss 325.0075 (397.8047)	attn_loss 318.0681 (388.6914)	hidden_loss 6.9395 (9.1133)	grad_norm 77.0145 (inf)	mem 26450MB
[2021-09-22 20:02:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][450/1251]	eta 0:16:14 lr 0.000100	time 1.2251 (1.2171)	loss 322.0447 (396.0648)	attn_loss 314.8062 (386.9910)	hidden_loss 7.2386 (9.0738)	grad_norm 72.8576 (inf)	mem 26450MB
[2021-09-22 20:02:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][460/1251]	eta 0:16:02 lr 0.000100	time 1.1990 (1.2167)	loss 308.1100 (394.3827)	attn_loss 300.9540 (385.3467)	hidden_loss 7.1560 (9.0360)	grad_norm 65.0002 (inf)	mem 26450MB
[2021-09-22 20:03:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][470/1251]	eta 0:15:49 lr 0.000100	time 1.2229 (1.2164)	loss 311.3902 (392.7306)	attn_loss 303.8401 (383.7284)	hidden_loss 7.5501 (9.0022)	grad_norm 66.8190 (inf)	mem 26450MB
[2021-09-22 20:03:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][480/1251]	eta 0:15:37 lr 0.000100	time 1.2130 (1.2160)	loss 320.8361 (391.1236)	attn_loss 313.9933 (382.1573)	hidden_loss 6.8428 (8.9663)	grad_norm 58.2977 (inf)	mem 26450MB
[2021-09-22 20:03:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][490/1251]	eta 0:15:25 lr 0.000100	time 1.1886 (1.2156)	loss 310.2118 (389.5727)	attn_loss 303.2508 (380.6423)	hidden_loss 6.9610 (8.9303)	grad_norm 64.3079 (inf)	mem 26450MB
[2021-09-22 20:03:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][500/1251]	eta 0:15:12 lr 0.000100	time 1.2110 (1.2152)	loss 303.2292 (388.0737)	attn_loss 296.2899 (379.1783)	hidden_loss 6.9393 (8.8954)	grad_norm 61.8224 (inf)	mem 26450MB
[2021-09-22 20:03:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][510/1251]	eta 0:15:00 lr 0.000100	time 1.2001 (1.2149)	loss 317.0390 (386.5742)	attn_loss 309.9833 (377.7144)	hidden_loss 7.0557 (8.8598)	grad_norm 54.9352 (inf)	mem 26450MB
[2021-09-22 20:04:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][520/1251]	eta 0:14:47 lr 0.000100	time 1.2161 (1.2146)	loss 316.2154 (385.1464)	attn_loss 309.1876 (376.3216)	hidden_loss 7.0278 (8.8247)	grad_norm 68.7560 (inf)	mem 26450MB
[2021-09-22 20:04:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][530/1251]	eta 0:14:35 lr 0.000100	time 1.1883 (1.2144)	loss 304.1754 (383.7180)	attn_loss 296.8575 (374.9264)	hidden_loss 7.3179 (8.7916)	grad_norm 54.9926 (inf)	mem 26450MB
[2021-09-22 20:04:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][540/1251]	eta 0:14:23 lr 0.000100	time 1.2013 (1.2142)	loss 304.6230 (382.3719)	attn_loss 297.0506 (373.6122)	hidden_loss 7.5723 (8.7598)	grad_norm 51.6713 (inf)	mem 26450MB
[2021-09-22 20:04:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][550/1251]	eta 0:14:10 lr 0.000100	time 1.2116 (1.2139)	loss 313.8316 (381.1081)	attn_loss 307.0104 (372.3819)	hidden_loss 6.8212 (8.7262)	grad_norm 51.9739 (inf)	mem 26450MB
[2021-09-22 20:04:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][560/1251]	eta 0:13:58 lr 0.000100	time 1.2017 (1.2137)	loss 300.1623 (379.8059)	attn_loss 293.2636 (371.1118)	hidden_loss 6.8987 (8.6940)	grad_norm 55.1312 (inf)	mem 26450MB
[2021-09-22 20:05:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][570/1251]	eta 0:13:46 lr 0.000100	time 1.1999 (1.2135)	loss 305.8315 (378.5214)	attn_loss 298.8455 (369.8566)	hidden_loss 6.9860 (8.6648)	grad_norm 51.4734 (inf)	mem 26450MB
[2021-09-22 20:05:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][580/1251]	eta 0:13:34 lr 0.000100	time 1.1838 (1.2132)	loss 314.3452 (377.3260)	attn_loss 307.5635 (368.6915)	hidden_loss 6.7817 (8.6345)	grad_norm 59.1972 (inf)	mem 26450MB
[2021-09-22 20:05:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][590/1251]	eta 0:13:21 lr 0.000100	time 1.2003 (1.2131)	loss 303.5829 (376.1251)	attn_loss 296.6545 (367.5191)	hidden_loss 6.9284 (8.6060)	grad_norm 53.4671 (inf)	mem 26450MB
[2021-09-22 20:05:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][600/1251]	eta 0:13:09 lr 0.000100	time 1.2091 (1.2128)	loss 302.5129 (374.9494)	attn_loss 295.8605 (366.3722)	hidden_loss 6.6524 (8.5772)	grad_norm 57.3585 (inf)	mem 26450MB
[2021-09-22 20:05:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][610/1251]	eta 0:12:57 lr 0.000100	time 1.1993 (1.2126)	loss 310.4159 (373.8208)	attn_loss 303.5574 (365.2741)	hidden_loss 6.8585 (8.5467)	grad_norm 48.7518 (inf)	mem 26450MB
[2021-09-22 20:06:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][620/1251]	eta 0:12:45 lr 0.000100	time 1.1999 (1.2124)	loss 309.9649 (372.7143)	attn_loss 303.1620 (364.1933)	hidden_loss 6.8030 (8.5210)	grad_norm 54.8824 (inf)	mem 26450MB
[2021-09-22 20:06:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][630/1251]	eta 0:12:32 lr 0.000100	time 1.1876 (1.2121)	loss 295.1042 (371.6253)	attn_loss 288.0856 (363.1294)	hidden_loss 7.0185 (8.4959)	grad_norm 50.9984 (inf)	mem 26450MB
[2021-09-22 20:06:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][640/1251]	eta 0:12:20 lr 0.000100	time 1.1950 (1.2119)	loss 300.4238 (370.5613)	attn_loss 293.5555 (362.0906)	hidden_loss 6.8683 (8.4707)	grad_norm 53.5379 (inf)	mem 26450MB
[2021-09-22 20:06:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][650/1251]	eta 0:12:08 lr 0.000100	time 1.2062 (1.2118)	loss 307.7169 (369.5214)	attn_loss 301.0403 (361.0747)	hidden_loss 6.6766 (8.4467)	grad_norm 61.2400 (inf)	mem 26450MB
[2021-09-22 20:06:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][660/1251]	eta 0:11:56 lr 0.000100	time 1.2040 (1.2116)	loss 298.0819 (368.4895)	attn_loss 291.1724 (360.0674)	hidden_loss 6.9095 (8.4221)	grad_norm 50.2420 (inf)	mem 26450MB
[2021-09-22 20:07:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][670/1251]	eta 0:11:43 lr 0.000100	time 1.2036 (1.2114)	loss 304.7639 (367.4774)	attn_loss 298.1899 (359.0804)	hidden_loss 6.5740 (8.3970)	grad_norm 51.0938 (inf)	mem 26450MB
[2021-09-22 20:07:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][680/1251]	eta 0:11:31 lr 0.000100	time 1.1871 (1.2112)	loss 304.6099 (366.5065)	attn_loss 297.9968 (358.1324)	hidden_loss 6.6131 (8.3741)	grad_norm 54.3051 (inf)	mem 26450MB
[2021-09-22 20:07:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][690/1251]	eta 0:11:19 lr 0.000100	time 1.1977 (1.2111)	loss 294.5596 (365.5251)	attn_loss 287.1780 (357.1715)	hidden_loss 7.3816 (8.3537)	grad_norm 49.2809 (inf)	mem 26450MB
[2021-09-22 20:07:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][700/1251]	eta 0:11:07 lr 0.000100	time 1.2003 (1.2110)	loss 292.3411 (364.5161)	attn_loss 285.6347 (356.1853)	hidden_loss 6.7064 (8.3308)	grad_norm 52.7395 (inf)	mem 26450MB
[2021-09-22 20:07:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][710/1251]	eta 0:10:55 lr 0.000100	time 1.1970 (1.2109)	loss 294.9851 (363.5661)	attn_loss 288.2372 (355.2582)	hidden_loss 6.7479 (8.3079)	grad_norm 46.6925 (inf)	mem 26450MB
[2021-09-22 20:08:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][720/1251]	eta 0:10:42 lr 0.000100	time 1.1987 (1.2108)	loss 299.5670 (362.6469)	attn_loss 292.9436 (354.3589)	hidden_loss 6.6235 (8.2881)	grad_norm 54.5199 (inf)	mem 26450MB
[2021-09-22 20:08:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][730/1251]	eta 0:10:30 lr 0.000100	time 1.2117 (1.2106)	loss 288.9669 (361.7494)	attn_loss 282.1235 (353.4831)	hidden_loss 6.8434 (8.2663)	grad_norm 51.1266 (inf)	mem 26450MB
[2021-09-22 20:08:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][740/1251]	eta 0:10:18 lr 0.000100	time 1.2020 (1.2105)	loss 289.6494 (360.8277)	attn_loss 283.1018 (352.5833)	hidden_loss 6.5476 (8.2444)	grad_norm 47.0313 (inf)	mem 26450MB
[2021-09-22 20:08:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][750/1251]	eta 0:10:06 lr 0.000100	time 1.2032 (1.2103)	loss 291.1955 (359.9203)	attn_loss 284.6841 (351.6962)	hidden_loss 6.5114 (8.2241)	grad_norm 45.6911 (inf)	mem 26450MB
[2021-09-22 20:08:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][760/1251]	eta 0:09:54 lr 0.000100	time 1.2210 (1.2102)	loss 288.2838 (359.0342)	attn_loss 281.9105 (350.8306)	hidden_loss 6.3734 (8.2036)	grad_norm 50.8880 (inf)	mem 26450MB
[2021-09-22 20:09:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][770/1251]	eta 0:09:42 lr 0.000100	time 1.1963 (1.2100)	loss 298.3897 (358.1951)	attn_loss 291.8254 (350.0102)	hidden_loss 6.5643 (8.1849)	grad_norm 51.2925 (inf)	mem 26450MB
[2021-09-22 20:09:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][780/1251]	eta 0:09:29 lr 0.000100	time 1.2024 (1.2099)	loss 297.1451 (357.3655)	attn_loss 290.8167 (349.2008)	hidden_loss 6.3284 (8.1648)	grad_norm 47.3811 (inf)	mem 26450MB
[2021-09-22 20:09:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][790/1251]	eta 0:09:17 lr 0.000100	time 1.1855 (1.2097)	loss 288.4282 (356.5389)	attn_loss 281.2046 (348.3917)	hidden_loss 7.2236 (8.1472)	grad_norm 52.3244 (inf)	mem 26450MB
[2021-09-22 20:09:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][800/1251]	eta 0:09:05 lr 0.000100	time 1.1982 (1.2096)	loss 297.7385 (355.7435)	attn_loss 291.3820 (347.6164)	hidden_loss 6.3565 (8.1271)	grad_norm 53.3072 (inf)	mem 26450MB
[2021-09-22 20:09:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][810/1251]	eta 0:08:53 lr 0.000100	time 1.2111 (1.2095)	loss 291.0006 (354.9404)	attn_loss 284.3798 (346.8314)	hidden_loss 6.6208 (8.1091)	grad_norm 41.7284 (inf)	mem 26450MB
[2021-09-22 20:10:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][820/1251]	eta 0:08:41 lr 0.000100	time 1.1943 (1.2094)	loss 295.9156 (354.1549)	attn_loss 289.3455 (346.0641)	hidden_loss 6.5701 (8.0908)	grad_norm 52.0108 (inf)	mem 26450MB
[2021-09-22 20:10:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][830/1251]	eta 0:08:29 lr 0.000100	time 1.1965 (1.2093)	loss 285.9469 (353.3620)	attn_loss 279.4542 (345.2890)	hidden_loss 6.4928 (8.0731)	grad_norm 47.8313 (inf)	mem 26450MB
[2021-09-22 20:10:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][840/1251]	eta 0:08:16 lr 0.000100	time 1.1984 (1.2092)	loss 294.6053 (352.6413)	attn_loss 287.9978 (344.5849)	hidden_loss 6.6075 (8.0564)	grad_norm 55.5387 (inf)	mem 26450MB
[2021-09-22 20:10:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][850/1251]	eta 0:08:04 lr 0.000100	time 1.1858 (1.2091)	loss 293.1669 (351.9042)	attn_loss 286.8319 (343.8653)	hidden_loss 6.3350 (8.0388)	grad_norm 47.0057 (inf)	mem 26450MB
[2021-09-22 20:10:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][860/1251]	eta 0:07:52 lr 0.000100	time 1.1975 (1.2091)	loss 287.1475 (351.1904)	attn_loss 280.7380 (343.1686)	hidden_loss 6.4094 (8.0218)	grad_norm 47.6398 (inf)	mem 26450MB
[2021-09-22 20:11:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][870/1251]	eta 0:07:40 lr 0.000100	time 1.1821 (1.2090)	loss 292.7933 (350.4595)	attn_loss 286.2359 (342.4538)	hidden_loss 6.5574 (8.0057)	grad_norm 47.9001 (inf)	mem 26450MB
[2021-09-22 20:11:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][880/1251]	eta 0:07:28 lr 0.000100	time 1.2046 (1.2088)	loss 293.7879 (349.7489)	attn_loss 287.3654 (341.7581)	hidden_loss 6.4225 (7.9909)	grad_norm 46.4613 (inf)	mem 26450MB
[2021-09-22 20:11:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][890/1251]	eta 0:07:16 lr 0.000100	time 1.2047 (1.2087)	loss 284.1266 (349.0519)	attn_loss 277.6078 (341.0783)	hidden_loss 6.5187 (7.9737)	grad_norm 56.5270 (inf)	mem 26450MB
[2021-09-22 20:11:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][900/1251]	eta 0:07:04 lr 0.000100	time 1.2140 (1.2086)	loss 287.4669 (348.3701)	attn_loss 280.7853 (340.4110)	hidden_loss 6.6816 (7.9591)	grad_norm 42.2585 (inf)	mem 26450MB
[2021-09-22 20:11:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][910/1251]	eta 0:06:52 lr 0.000100	time 1.2119 (1.2085)	loss 282.6399 (347.6934)	attn_loss 276.0960 (339.7493)	hidden_loss 6.5439 (7.9442)	grad_norm 53.1708 (inf)	mem 26450MB
[2021-09-22 20:12:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][920/1251]	eta 0:06:39 lr 0.000100	time 1.1952 (1.2083)	loss 292.0911 (347.0451)	attn_loss 285.3726 (339.1162)	hidden_loss 6.7185 (7.9290)	grad_norm 46.0614 (inf)	mem 26450MB
[2021-09-22 20:12:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][930/1251]	eta 0:06:27 lr 0.000100	time 1.1958 (1.2082)	loss 282.5195 (346.3893)	attn_loss 276.0137 (338.4763)	hidden_loss 6.5058 (7.9130)	grad_norm 50.2966 (inf)	mem 26450MB
[2021-09-22 20:12:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][940/1251]	eta 0:06:15 lr 0.000100	time 1.2071 (1.2081)	loss 291.1453 (345.7536)	attn_loss 284.7175 (337.8551)	hidden_loss 6.4279 (7.8984)	grad_norm 59.3364 (inf)	mem 26450MB
[2021-09-22 20:12:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][950/1251]	eta 0:06:03 lr 0.000100	time 1.2078 (1.2081)	loss 280.4368 (345.1055)	attn_loss 273.9827 (337.2209)	hidden_loss 6.4541 (7.8846)	grad_norm 50.5225 (inf)	mem 26450MB
[2021-09-22 20:12:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][960/1251]	eta 0:05:51 lr 0.000100	time 1.1936 (1.2079)	loss 280.7415 (344.4824)	attn_loss 274.2336 (336.6129)	hidden_loss 6.5078 (7.8696)	grad_norm 50.9577 (inf)	mem 26450MB
[2021-09-22 20:13:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][970/1251]	eta 0:05:39 lr 0.000100	time 1.2072 (1.2078)	loss 279.8499 (343.8377)	attn_loss 272.9221 (335.9821)	hidden_loss 6.9278 (7.8556)	grad_norm 57.2437 (inf)	mem 26450MB
[2021-09-22 20:13:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][980/1251]	eta 0:05:27 lr 0.000100	time 1.2042 (1.2077)	loss 283.0219 (343.2289)	attn_loss 276.8344 (335.3874)	hidden_loss 6.1876 (7.8414)	grad_norm 52.5831 (inf)	mem 26450MB
[2021-09-22 20:13:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][990/1251]	eta 0:05:15 lr 0.000100	time 1.2038 (1.2077)	loss 278.8578 (342.6097)	attn_loss 272.2443 (334.7824)	hidden_loss 6.6135 (7.8273)	grad_norm 54.2407 (inf)	mem 26450MB
[2021-09-22 20:13:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1000/1251]	eta 0:05:03 lr 0.000100	time 1.1943 (1.2076)	loss 285.1786 (342.0132)	attn_loss 278.1033 (334.1977)	hidden_loss 7.0752 (7.8155)	grad_norm 44.7427 (inf)	mem 26450MB
[2021-09-22 20:13:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1010/1251]	eta 0:04:51 lr 0.000100	time 1.1843 (1.2075)	loss 278.5640 (341.4315)	attn_loss 271.9456 (333.6298)	hidden_loss 6.6183 (7.8017)	grad_norm 40.8265 (inf)	mem 26450MB
[2021-09-22 20:14:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1020/1251]	eta 0:04:38 lr 0.000100	time 1.2187 (1.2074)	loss 288.7587 (340.8879)	attn_loss 281.9857 (333.0984)	hidden_loss 6.7730 (7.7895)	grad_norm 55.1725 (inf)	mem 26450MB
[2021-09-22 20:14:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1030/1251]	eta 0:04:26 lr 0.000100	time 1.1997 (1.2073)	loss 278.8384 (340.2976)	attn_loss 272.2471 (332.5188)	hidden_loss 6.5913 (7.7788)	grad_norm 39.7729 (inf)	mem 26450MB
[2021-09-22 20:14:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1040/1251]	eta 0:04:14 lr 0.000100	time 1.1933 (1.2072)	loss 286.9915 (339.7694)	attn_loss 280.4981 (332.0027)	hidden_loss 6.4934 (7.7667)	grad_norm 44.9514 (inf)	mem 26450MB
[2021-09-22 20:14:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1050/1251]	eta 0:04:02 lr 0.000100	time 1.1960 (1.2071)	loss 278.5761 (339.2131)	attn_loss 271.5662 (331.4574)	hidden_loss 7.0099 (7.7557)	grad_norm 59.8315 (inf)	mem 26450MB
[2021-09-22 20:14:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1060/1251]	eta 0:03:50 lr 0.000100	time 1.2207 (1.2070)	loss 283.7441 (338.6655)	attn_loss 277.3481 (330.9220)	hidden_loss 6.3960 (7.7435)	grad_norm 61.8849 (inf)	mem 26450MB
[2021-09-22 20:15:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1070/1251]	eta 0:03:38 lr 0.000100	time 1.1953 (1.2070)	loss 276.3294 (338.1418)	attn_loss 270.0062 (330.4090)	hidden_loss 6.3233 (7.7328)	grad_norm 59.3808 (inf)	mem 26450MB
[2021-09-22 20:15:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1080/1251]	eta 0:03:26 lr 0.000100	time 1.1883 (1.2069)	loss 276.8940 (337.5922)	attn_loss 270.2993 (329.8694)	hidden_loss 6.5947 (7.7228)	grad_norm 46.0365 (inf)	mem 26450MB
[2021-09-22 20:15:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1090/1251]	eta 0:03:14 lr 0.000100	time 1.1943 (1.2069)	loss 277.7944 (337.0717)	attn_loss 271.1592 (329.3610)	hidden_loss 6.6352 (7.7106)	grad_norm 65.6921 (inf)	mem 26450MB
[2021-09-22 20:15:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1100/1251]	eta 0:03:02 lr 0.000100	time 1.1962 (1.2068)	loss 276.4326 (336.5630)	attn_loss 270.1419 (328.8623)	hidden_loss 6.2907 (7.7008)	grad_norm 59.3843 (inf)	mem 26450MB
[2021-09-22 20:15:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1110/1251]	eta 0:02:50 lr 0.000100	time 1.2117 (1.2067)	loss 285.8154 (336.0709)	attn_loss 279.2921 (328.3806)	hidden_loss 6.5233 (7.6902)	grad_norm 53.5132 (inf)	mem 26450MB
[2021-09-22 20:16:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1120/1251]	eta 0:02:38 lr 0.000100	time 1.2107 (1.2067)	loss 277.1457 (335.5616)	attn_loss 270.6801 (327.8822)	hidden_loss 6.4656 (7.6794)	grad_norm 90.1794 (inf)	mem 26450MB
[2021-09-22 20:16:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1130/1251]	eta 0:02:25 lr 0.000100	time 1.1885 (1.2066)	loss 284.4506 (335.0684)	attn_loss 277.8563 (327.3989)	hidden_loss 6.5944 (7.6695)	grad_norm 55.9222 (inf)	mem 26450MB
[2021-09-22 20:16:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1140/1251]	eta 0:02:13 lr 0.000100	time 1.1840 (1.2065)	loss 273.3307 (334.5645)	attn_loss 267.2077 (326.9053)	hidden_loss 6.1230 (7.6591)	grad_norm 56.4604 (inf)	mem 26450MB
[2021-09-22 20:16:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1150/1251]	eta 0:02:01 lr 0.000100	time 1.1921 (1.2064)	loss 276.5656 (334.0818)	attn_loss 270.1156 (326.4313)	hidden_loss 6.4501 (7.6505)	grad_norm 56.9257 (inf)	mem 26450MB
[2021-09-22 20:16:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1160/1251]	eta 0:01:49 lr 0.000100	time 1.2061 (1.2064)	loss 283.2944 (333.6115)	attn_loss 277.0424 (325.9715)	hidden_loss 6.2521 (7.6399)	grad_norm 64.3889 (inf)	mem 26450MB
[2021-09-22 20:17:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1170/1251]	eta 0:01:37 lr 0.000100	time 1.1940 (1.2063)	loss 274.7294 (333.1400)	attn_loss 267.9058 (325.5085)	hidden_loss 6.8236 (7.6315)	grad_norm 54.3482 (inf)	mem 26450MB
[2021-09-22 20:17:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1180/1251]	eta 0:01:25 lr 0.000100	time 1.1822 (1.2062)	loss 280.8158 (332.6976)	attn_loss 273.5234 (325.0747)	hidden_loss 7.2924 (7.6229)	grad_norm 83.4577 (inf)	mem 26450MB
[2021-09-22 20:17:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1190/1251]	eta 0:01:13 lr 0.000100	time 1.1931 (1.2062)	loss 273.3856 (332.2436)	attn_loss 266.9376 (324.6310)	hidden_loss 6.4480 (7.6125)	grad_norm 58.2543 (inf)	mem 26450MB
[2021-09-22 20:17:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1200/1251]	eta 0:01:01 lr 0.000100	time 1.1971 (1.2061)	loss 282.9163 (331.8076)	attn_loss 276.4784 (324.2034)	hidden_loss 6.4379 (7.6042)	grad_norm 71.0885 (inf)	mem 26450MB
[2021-09-22 20:17:59 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1210/1251]	eta 0:00:49 lr 0.000100	time 1.2067 (1.2061)	loss 278.1531 (331.3730)	attn_loss 271.5064 (323.7775)	hidden_loss 6.6467 (7.5955)	grad_norm 50.3374 (inf)	mem 26450MB
[2021-09-22 20:18:11 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1220/1251]	eta 0:00:37 lr 0.000100	time 1.2059 (1.2060)	loss 277.9909 (330.9438)	attn_loss 271.5426 (323.3574)	hidden_loss 6.4483 (7.5864)	grad_norm 53.0203 (inf)	mem 26450MB
[2021-09-22 20:18:23 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1230/1251]	eta 0:00:25 lr 0.000100	time 1.1989 (1.2059)	loss 278.1932 (330.4934)	attn_loss 271.7457 (322.9162)	hidden_loss 6.4475 (7.5772)	grad_norm 57.9879 (inf)	mem 26450MB
[2021-09-22 20:18:35 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1240/1251]	eta 0:00:13 lr 0.000100	time 1.1830 (1.2058)	loss 283.1211 (330.0759)	attn_loss 276.6143 (322.5078)	hidden_loss 6.5069 (7.5681)	grad_norm 78.7028 (inf)	mem 26450MB
[2021-09-22 20:18:47 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][1250/1251]	eta 0:00:01 lr 0.000100	time 1.2024 (1.2058)	loss 284.4858 (329.6392)	attn_loss 278.2095 (322.0798)	hidden_loss 6.2763 (7.5595)	grad_norm 168.8725 (inf)	mem 26450MB
[2021-09-22 20:18:47 swin_tiny_patch4_window7_224] (main.py 371): INFO EPOCH 0 training takes 0:25:08
[2021-09-22 20:18:47 swin_tiny_patch4_window7_224] (utils.py 63): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth saving......
[2021-09-22 20:18:49 swin_tiny_patch4_window7_224] (utils.py 65): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_0.pth saved !!!
[2021-09-22 20:18:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][0/1251]	eta 2:42:24 lr 0.000100	time 7.7896 (7.7896)	loss 272.2333 (272.2333)	attn_loss 265.7473 (265.7473)	hidden_loss 6.4860 (6.4860)	grad_norm 84.7158 (84.7158)	mem 26450MB
[2021-09-22 20:19:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][10/1251]	eta 0:37:13 lr 0.000100	time 1.1971 (1.7999)	loss 281.1414 (275.8888)	attn_loss 274.5304 (269.3696)	hidden_loss 6.6110 (6.5192)	grad_norm 78.6715 (81.0102)	mem 26450MB
[2021-09-22 20:19:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][20/1251]	eta 0:31:02 lr 0.000100	time 1.1940 (1.5129)	loss 272.7543 (276.4783)	attn_loss 265.9531 (269.9646)	hidden_loss 6.8012 (6.5136)	grad_norm 57.2668 (75.1327)	mem 26450MB
[2021-09-22 20:19:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][30/1251]	eta 0:28:44 lr 0.000100	time 1.1863 (1.4123)	loss 272.4976 (277.3824)	attn_loss 266.0051 (270.9209)	hidden_loss 6.4926 (6.4615)	grad_norm 71.2427 (70.5768)	mem 26450MB
[2021-09-22 20:19:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][40/1251]	eta 0:27:27 lr 0.000100	time 1.1961 (1.3602)	loss 270.3166 (276.4641)	attn_loss 263.9502 (270.0162)	hidden_loss 6.3664 (6.4480)	grad_norm 107.2713 (71.2071)	mem 26450MB
[2021-09-22 20:19:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][50/1251]	eta 0:26:35 lr 0.000100	time 1.1932 (1.3289)	loss 277.5057 (276.8128)	attn_loss 271.0097 (270.3719)	hidden_loss 6.4960 (6.4409)	grad_norm 62.8923 (72.5076)	mem 26450MB
[2021-09-22 20:20:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][60/1251]	eta 0:25:57 lr 0.000100	time 1.1890 (1.3077)	loss 271.3357 (276.7084)	attn_loss 265.4225 (270.2923)	hidden_loss 5.9132 (6.4161)	grad_norm 57.6688 (75.5193)	mem 26450MB
[2021-09-22 20:20:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][70/1251]	eta 0:25:26 lr 0.000100	time 1.2066 (1.2926)	loss 279.5834 (276.5663)	attn_loss 273.1670 (270.1239)	hidden_loss 6.4164 (6.4424)	grad_norm 58.4988 (76.2744)	mem 26450MB
[2021-09-22 20:20:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][80/1251]	eta 0:24:59 lr 0.000100	time 1.2178 (1.2807)	loss 279.4029 (276.4620)	attn_loss 272.8173 (270.0250)	hidden_loss 6.5856 (6.4369)	grad_norm 75.1830 (76.1779)	mem 26450MB
[2021-09-22 20:20:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][90/1251]	eta 0:24:36 lr 0.000100	time 1.1964 (1.2714)	loss 268.8243 (276.2736)	attn_loss 262.7469 (269.8357)	hidden_loss 6.0774 (6.4379)	grad_norm 99.7214 (74.5062)	mem 26450MB
[2021-09-22 20:20:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][100/1251]	eta 0:24:15 lr 0.000100	time 1.1888 (1.2647)	loss 281.7923 (276.1320)	attn_loss 275.5443 (269.6825)	hidden_loss 6.2480 (6.4496)	grad_norm 91.4402 (73.0172)	mem 26450MB
[2021-09-22 20:21:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][110/1251]	eta 0:23:56 lr 0.000100	time 1.1908 (1.2589)	loss 274.7917 (276.0197)	attn_loss 267.7964 (269.5714)	hidden_loss 6.9952 (6.4483)	grad_norm 71.5608 (72.9545)	mem 26450MB
[2021-09-22 20:21:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][120/1251]	eta 0:23:38 lr 0.000100	time 1.2006 (1.2542)	loss 268.9648 (275.6058)	attn_loss 262.6728 (269.1713)	hidden_loss 6.2920 (6.4345)	grad_norm 57.5152 (74.0581)	mem 26450MB
[2021-09-22 20:21:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][130/1251]	eta 0:23:21 lr 0.000100	time 1.2137 (1.2503)	loss 268.1748 (275.3609)	attn_loss 262.3242 (268.9280)	hidden_loss 5.8506 (6.4329)	grad_norm 76.8220 (72.9121)	mem 26450MB
[2021-09-22 20:21:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][140/1251]	eta 0:23:04 lr 0.000100	time 1.2187 (1.2466)	loss 273.4608 (275.4636)	attn_loss 266.9789 (269.0369)	hidden_loss 6.4819 (6.4267)	grad_norm 83.8698 (72.9802)	mem 26450MB
[2021-09-22 20:21:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][150/1251]	eta 0:22:49 lr 0.000100	time 1.2053 (1.2437)	loss 279.5141 (275.4523)	attn_loss 273.2941 (269.0370)	hidden_loss 6.2199 (6.4154)	grad_norm 83.7060 (72.8716)	mem 26450MB
[2021-09-22 20:22:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][160/1251]	eta 0:22:33 lr 0.000100	time 1.2011 (1.2409)	loss 277.8182 (275.5881)	attn_loss 271.5062 (269.1749)	hidden_loss 6.3120 (6.4132)	grad_norm 77.4579 (72.5067)	mem 26450MB
[2021-09-22 20:22:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][170/1251]	eta 0:22:18 lr 0.000100	time 1.2010 (1.2385)	loss 268.9179 (275.3417)	attn_loss 262.6947 (268.9332)	hidden_loss 6.2232 (6.4085)	grad_norm 83.5038 (73.0941)	mem 26450MB
[2021-09-22 20:22:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][180/1251]	eta 0:22:04 lr 0.000100	time 1.1886 (1.2363)	loss 279.8074 (275.3669)	attn_loss 273.3669 (268.9637)	hidden_loss 6.4404 (6.4032)	grad_norm 63.5585 (73.2736)	mem 26450MB
[2021-09-22 20:22:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][190/1251]	eta 0:21:49 lr 0.000100	time 1.1926 (1.2344)	loss 280.5574 (275.2969)	attn_loss 274.1725 (268.8940)	hidden_loss 6.3849 (6.4029)	grad_norm 57.1298 (73.5330)	mem 26450MB
[2021-09-22 20:22:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][200/1251]	eta 0:21:35 lr 0.000100	time 1.1975 (1.2326)	loss 270.4604 (275.2931)	attn_loss 263.9655 (268.8914)	hidden_loss 6.4949 (6.4017)	grad_norm 92.4981 (73.6632)	mem 26450MB
[2021-09-22 20:23:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][210/1251]	eta 0:21:21 lr 0.000100	time 1.2080 (1.2309)	loss 279.3503 (275.2694)	attn_loss 272.7641 (268.8654)	hidden_loss 6.5862 (6.4040)	grad_norm 77.3918 (73.4268)	mem 26450MB
[2021-09-22 20:23:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][220/1251]	eta 0:21:07 lr 0.000100	time 1.1887 (1.2294)	loss 269.0222 (275.0713)	attn_loss 262.7144 (268.6657)	hidden_loss 6.3077 (6.4055)	grad_norm 84.7240 (73.0997)	mem 26450MB
[2021-09-22 20:23:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][230/1251]	eta 0:20:53 lr 0.000100	time 1.2017 (1.2282)	loss 271.8933 (274.9594)	attn_loss 265.3353 (268.5506)	hidden_loss 6.5580 (6.4087)	grad_norm 94.0685 (73.5840)	mem 26450MB
[2021-09-22 20:23:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][240/1251]	eta 0:20:40 lr 0.000100	time 1.1908 (1.2269)	loss 279.2468 (274.8624)	attn_loss 272.7803 (268.4503)	hidden_loss 6.4665 (6.4121)	grad_norm 76.8616 (75.7161)	mem 26450MB
[2021-09-22 20:23:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][250/1251]	eta 0:20:26 lr 0.000100	time 1.1942 (1.2257)	loss 276.9062 (274.7960)	attn_loss 270.6876 (268.3874)	hidden_loss 6.2187 (6.4086)	grad_norm 101.0484 (77.3123)	mem 26450MB
[2021-09-22 20:24:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][260/1251]	eta 0:20:13 lr 0.000100	time 1.2173 (1.2246)	loss 277.8693 (274.6659)	attn_loss 271.5141 (268.2525)	hidden_loss 6.3552 (6.4134)	grad_norm 89.3702 (77.9265)	mem 26450MB
[2021-09-22 20:24:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][270/1251]	eta 0:20:00 lr 0.000100	time 1.1966 (1.2237)	loss 278.5046 (274.6190)	attn_loss 271.9897 (268.2068)	hidden_loss 6.5149 (6.4122)	grad_norm 88.3624 (78.3345)	mem 26450MB
[2021-09-22 20:24:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][280/1251]	eta 0:19:47 lr 0.000100	time 1.1963 (1.2228)	loss 265.8318 (274.5155)	attn_loss 259.3080 (268.0980)	hidden_loss 6.5238 (6.4175)	grad_norm 52.4128 (78.4609)	mem 26450MB
[2021-09-22 20:24:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][290/1251]	eta 0:19:34 lr 0.000100	time 1.1909 (1.2220)	loss 265.9683 (274.3313)	attn_loss 259.4944 (267.9162)	hidden_loss 6.4738 (6.4151)	grad_norm 90.7847 (78.6869)	mem 26450MB
[2021-09-22 20:24:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][300/1251]	eta 0:19:21 lr 0.000100	time 1.1999 (1.2211)	loss 271.1724 (274.2149)	attn_loss 264.7467 (267.7972)	hidden_loss 6.4258 (6.4177)	grad_norm 45.3839 (78.3265)	mem 26450MB
[2021-09-22 20:25:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][310/1251]	eta 0:19:08 lr 0.000100	time 1.1987 (1.2206)	loss 265.0977 (274.0727)	attn_loss 258.8397 (267.6608)	hidden_loss 6.2579 (6.4119)	grad_norm 95.3042 (79.1812)	mem 26450MB
[2021-09-22 20:25:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][320/1251]	eta 0:18:55 lr 0.000100	time 1.1952 (1.2200)	loss 276.4282 (274.0122)	attn_loss 270.1342 (267.6030)	hidden_loss 6.2940 (6.4092)	grad_norm 76.6426 (79.3625)	mem 26450MB
[2021-09-22 20:25:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][330/1251]	eta 0:18:42 lr 0.000100	time 1.1906 (1.2193)	loss 278.2363 (273.9500)	attn_loss 271.7704 (267.5409)	hidden_loss 6.4659 (6.4091)	grad_norm 169.5478 (79.6186)	mem 26450MB
[2021-09-22 20:25:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][340/1251]	eta 0:18:30 lr 0.000100	time 1.1890 (1.2187)	loss 273.2292 (273.9789)	attn_loss 266.7051 (267.5703)	hidden_loss 6.5241 (6.4086)	grad_norm 122.9130 (80.9536)	mem 26450MB
[2021-09-22 20:25:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][350/1251]	eta 0:18:17 lr 0.000100	time 1.2024 (1.2180)	loss 265.3802 (273.9108)	attn_loss 258.8217 (267.5009)	hidden_loss 6.5585 (6.4099)	grad_norm 101.3550 (81.7150)	mem 26450MB
[2021-09-22 20:26:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][360/1251]	eta 0:18:04 lr 0.000100	time 1.1948 (1.2176)	loss 277.4299 (273.8228)	attn_loss 271.1732 (267.4128)	hidden_loss 6.2567 (6.4100)	grad_norm 63.0846 (81.6865)	mem 26450MB
[2021-09-22 20:26:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][370/1251]	eta 0:17:52 lr 0.000100	time 1.1873 (1.2170)	loss 265.6646 (273.7383)	attn_loss 259.2743 (267.3278)	hidden_loss 6.3903 (6.4105)	grad_norm 61.4536 (81.2155)	mem 26450MB
[2021-09-22 20:26:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][380/1251]	eta 0:17:39 lr 0.000100	time 1.1948 (1.2164)	loss 269.7817 (273.6906)	attn_loss 263.1980 (267.2821)	hidden_loss 6.5837 (6.4084)	grad_norm 56.5339 (80.7721)	mem 26450MB
[2021-09-22 20:26:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][390/1251]	eta 0:17:27 lr 0.000100	time 1.1861 (1.2161)	loss 265.1751 (273.5830)	attn_loss 258.0025 (267.1730)	hidden_loss 7.1727 (6.4100)	grad_norm 74.7282 (80.4929)	mem 26450MB
[2021-09-22 20:26:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][400/1251]	eta 0:17:14 lr 0.000100	time 1.1944 (1.2156)	loss 268.8165 (273.4597)	attn_loss 262.3623 (267.0489)	hidden_loss 6.4542 (6.4108)	grad_norm 60.2995 (80.1856)	mem 26450MB
[2021-09-22 20:27:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][410/1251]	eta 0:17:01 lr 0.000100	time 1.1925 (1.2152)	loss 266.5886 (273.3242)	attn_loss 259.4057 (266.9105)	hidden_loss 7.1829 (6.4137)	grad_norm 197.4919 (81.1373)	mem 26450MB
[2021-09-22 20:27:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][420/1251]	eta 0:16:49 lr 0.000100	time 1.1941 (1.2149)	loss 264.0824 (273.2918)	attn_loss 257.9026 (266.8810)	hidden_loss 6.1798 (6.4108)	grad_norm 88.8107 (82.1990)	mem 26450MB
[2021-09-22 20:27:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][430/1251]	eta 0:16:37 lr 0.000100	time 1.2010 (1.2147)	loss 263.4571 (273.2231)	attn_loss 257.2316 (266.8117)	hidden_loss 6.2254 (6.4114)	grad_norm 91.3214 (82.8538)	mem 26450MB
[2021-09-22 20:27:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][440/1251]	eta 0:16:24 lr 0.000100	time 1.1833 (1.2142)	loss 273.7305 (273.1811)	attn_loss 267.4415 (266.7701)	hidden_loss 6.2890 (6.4110)	grad_norm 55.1245 (83.1951)	mem 26450MB
[2021-09-22 20:27:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][450/1251]	eta 0:16:12 lr 0.000100	time 1.1814 (1.2138)	loss 264.7004 (273.0955)	attn_loss 258.5517 (266.6867)	hidden_loss 6.1488 (6.4088)	grad_norm 75.7040 (83.8374)	mem 26450MB
[2021-09-22 20:28:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][460/1251]	eta 0:16:00 lr 0.000100	time 1.1911 (1.2137)	loss 265.5556 (273.0405)	attn_loss 259.1564 (266.6313)	hidden_loss 6.3992 (6.4092)	grad_norm 71.9862 (83.6427)	mem 26450MB
[2021-09-22 20:28:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][470/1251]	eta 0:15:47 lr 0.000100	time 1.1863 (1.2133)	loss 262.6525 (272.9319)	attn_loss 256.5152 (266.5234)	hidden_loss 6.1374 (6.4085)	grad_norm 59.5899 (83.2703)	mem 26450MB
[2021-09-22 20:28:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][480/1251]	eta 0:15:35 lr 0.000100	time 1.1968 (1.2130)	loss 275.2081 (272.8860)	attn_loss 268.9580 (266.4749)	hidden_loss 6.2501 (6.4111)	grad_norm 52.9665 (83.8607)	mem 26450MB
[2021-09-22 20:28:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][490/1251]	eta 0:15:22 lr 0.000100	time 1.2064 (1.2128)	loss 275.5588 (272.7760)	attn_loss 268.7878 (266.3639)	hidden_loss 6.7710 (6.4121)	grad_norm 87.6059 (84.6019)	mem 26450MB
[2021-09-22 20:28:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][500/1251]	eta 0:15:10 lr 0.000100	time 1.1869 (1.2125)	loss 273.1708 (272.6756)	attn_loss 266.2947 (266.2650)	hidden_loss 6.8762 (6.4106)	grad_norm 70.8236 (84.4879)	mem 26450MB
[2021-09-22 20:29:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][510/1251]	eta 0:14:58 lr 0.000100	time 1.2072 (1.2122)	loss 261.4120 (272.5685)	attn_loss 255.3589 (266.1596)	hidden_loss 6.0530 (6.4089)	grad_norm 112.8691 (84.4067)	mem 26450MB
[2021-09-22 20:29:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][520/1251]	eta 0:14:45 lr 0.000100	time 1.1961 (1.2120)	loss 262.5363 (272.4928)	attn_loss 256.3520 (266.0835)	hidden_loss 6.1843 (6.4092)	grad_norm 77.2286 (84.3383)	mem 26450MB
[2021-09-22 20:29:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][530/1251]	eta 0:14:33 lr 0.000100	time 1.1900 (1.2117)	loss 261.6531 (272.3740)	attn_loss 255.0577 (265.9648)	hidden_loss 6.5954 (6.4091)	grad_norm 161.4157 (84.3788)	mem 26450MB
[2021-09-22 20:29:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][540/1251]	eta 0:14:21 lr 0.000100	time 1.1835 (1.2114)	loss 262.9881 (272.2798)	attn_loss 256.5533 (265.8721)	hidden_loss 6.4348 (6.4077)	grad_norm 132.5278 (84.9177)	mem 26450MB
[2021-09-22 20:29:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][550/1251]	eta 0:14:09 lr 0.000100	time 1.2142 (1.2111)	loss 265.0570 (272.2246)	attn_loss 258.3274 (265.8169)	hidden_loss 6.7296 (6.4078)	grad_norm 89.9566 (84.8681)	mem 26450MB
[2021-09-22 20:30:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][560/1251]	eta 0:13:56 lr 0.000100	time 1.2045 (1.2110)	loss 274.3283 (272.1243)	attn_loss 267.9250 (265.7176)	hidden_loss 6.4033 (6.4067)	grad_norm 124.7039 (85.0449)	mem 26450MB
[2021-09-22 20:30:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][570/1251]	eta 0:13:44 lr 0.000100	time 1.2125 (1.2107)	loss 262.7247 (272.0156)	attn_loss 256.2119 (265.6096)	hidden_loss 6.5128 (6.4060)	grad_norm 104.0302 (85.8253)	mem 26450MB
[2021-09-22 20:30:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][580/1251]	eta 0:13:32 lr 0.000100	time 1.1907 (1.2106)	loss 268.8027 (271.9286)	attn_loss 262.2779 (265.5245)	hidden_loss 6.5248 (6.4042)	grad_norm 120.5985 (86.4740)	mem 26450MB
[2021-09-22 20:30:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][590/1251]	eta 0:13:20 lr 0.000100	time 1.2154 (1.2104)	loss 267.0298 (271.8394)	attn_loss 260.6572 (265.4359)	hidden_loss 6.3725 (6.4035)	grad_norm 101.5536 (86.8215)	mem 26450MB
[2021-09-22 20:30:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][600/1251]	eta 0:13:07 lr 0.000100	time 1.1908 (1.2102)	loss 271.4911 (271.7251)	attn_loss 265.2012 (265.3222)	hidden_loss 6.2899 (6.4028)	grad_norm 61.8667 (87.1198)	mem 26450MB
[2021-09-22 20:31:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][610/1251]	eta 0:12:55 lr 0.000100	time 1.1934 (1.2100)	loss 263.4822 (271.6634)	attn_loss 257.2245 (265.2623)	hidden_loss 6.2577 (6.4012)	grad_norm 65.7169 (86.8473)	mem 26450MB
[2021-09-22 20:31:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][620/1251]	eta 0:12:43 lr 0.000100	time 1.1974 (1.2098)	loss 269.5276 (271.5930)	attn_loss 263.2515 (265.1921)	hidden_loss 6.2761 (6.4009)	grad_norm 63.7112 (86.8003)	mem 26450MB
[2021-09-22 20:31:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][630/1251]	eta 0:12:31 lr 0.000100	time 1.1975 (1.2097)	loss 269.3965 (271.5625)	attn_loss 262.5487 (265.1606)	hidden_loss 6.8478 (6.4018)	grad_norm 48.8138 (86.7237)	mem 26450MB
[2021-09-22 20:31:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][640/1251]	eta 0:12:19 lr 0.000100	time 1.2040 (1.2095)	loss 267.8405 (271.5025)	attn_loss 261.5467 (265.1011)	hidden_loss 6.2938 (6.4014)	grad_norm 59.4192 (87.3185)	mem 26450MB
[2021-09-22 20:31:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][650/1251]	eta 0:12:06 lr 0.000100	time 1.2223 (1.2094)	loss 268.6012 (271.4348)	attn_loss 262.1250 (265.0331)	hidden_loss 6.4762 (6.4017)	grad_norm 72.0667 (87.2460)	mem 26450MB
[2021-09-22 20:32:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][660/1251]	eta 0:11:54 lr 0.000100	time 1.1877 (1.2092)	loss 267.6598 (271.3058)	attn_loss 260.7150 (264.9027)	hidden_loss 6.9448 (6.4031)	grad_norm 128.9948 (87.2816)	mem 26450MB
[2021-09-22 20:32:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][670/1251]	eta 0:11:42 lr 0.000100	time 1.1876 (1.2091)	loss 267.0469 (271.2137)	attn_loss 260.0180 (264.8093)	hidden_loss 7.0289 (6.4044)	grad_norm 43.9824 (87.0731)	mem 26450MB
[2021-09-22 20:32:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][680/1251]	eta 0:11:30 lr 0.000100	time 1.2004 (1.2090)	loss 263.6919 (271.1402)	attn_loss 257.0083 (264.7356)	hidden_loss 6.6836 (6.4046)	grad_norm 61.0443 (86.9775)	mem 26450MB
[2021-09-22 20:32:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][690/1251]	eta 0:11:18 lr 0.000100	time 1.2127 (1.2088)	loss 260.5694 (271.0769)	attn_loss 254.2051 (264.6732)	hidden_loss 6.3643 (6.4037)	grad_norm 58.8655 (86.8835)	mem 26450MB
[2021-09-22 20:32:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][700/1251]	eta 0:11:05 lr 0.000100	time 1.1931 (1.2087)	loss 260.4905 (270.9672)	attn_loss 254.0671 (264.5648)	hidden_loss 6.4234 (6.4024)	grad_norm 71.1670 (86.8899)	mem 26450MB
[2021-09-22 20:33:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][710/1251]	eta 0:10:53 lr 0.000100	time 1.1906 (1.2085)	loss 272.8655 (270.8982)	attn_loss 266.2521 (264.4981)	hidden_loss 6.6133 (6.4001)	grad_norm 108.5253 (87.0731)	mem 26450MB
[2021-09-22 20:33:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][720/1251]	eta 0:10:41 lr 0.000100	time 1.2154 (1.2084)	loss 262.9041 (270.7885)	attn_loss 256.5344 (264.3895)	hidden_loss 6.3697 (6.3990)	grad_norm 114.9404 (87.1061)	mem 26450MB
[2021-09-22 20:33:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][730/1251]	eta 0:10:29 lr 0.000100	time 1.1962 (1.2082)	loss 260.3412 (270.6866)	attn_loss 253.8140 (264.2856)	hidden_loss 6.5272 (6.4010)	grad_norm 212.3838 (87.8701)	mem 26450MB
[2021-09-22 20:33:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][740/1251]	eta 0:10:17 lr 0.000100	time 1.1886 (1.2081)	loss 259.9174 (270.6092)	attn_loss 253.8833 (264.2106)	hidden_loss 6.0340 (6.3986)	grad_norm 151.1357 (88.6734)	mem 26450MB
[2021-09-22 20:33:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][750/1251]	eta 0:10:05 lr 0.000100	time 1.1849 (1.2079)	loss 270.2483 (270.5379)	attn_loss 263.8531 (264.1384)	hidden_loss 6.3951 (6.3995)	grad_norm 153.9102 (89.2953)	mem 26450MB
[2021-09-22 20:34:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][760/1251]	eta 0:09:53 lr 0.000100	time 1.1836 (1.2077)	loss 262.1897 (270.4767)	attn_loss 255.8605 (264.0788)	hidden_loss 6.3292 (6.3979)	grad_norm 89.6940 (89.5846)	mem 26450MB
[2021-09-22 20:34:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][770/1251]	eta 0:09:40 lr 0.000100	time 1.1818 (1.2077)	loss 259.6002 (270.3882)	attn_loss 253.3822 (263.9902)	hidden_loss 6.2179 (6.3980)	grad_norm 59.2887 (89.7434)	mem 26450MB
[2021-09-22 20:34:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][780/1251]	eta 0:09:28 lr 0.000100	time 1.1983 (1.2076)	loss 259.2741 (270.3009)	attn_loss 252.8922 (263.9032)	hidden_loss 6.3819 (6.3978)	grad_norm 154.3832 (90.6167)	mem 26450MB
[2021-09-22 20:34:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][790/1251]	eta 0:09:16 lr 0.000100	time 1.1968 (1.2074)	loss 259.4652 (270.2398)	attn_loss 253.2907 (263.8429)	hidden_loss 6.1746 (6.3970)	grad_norm 129.3984 (91.2065)	mem 26450MB
[2021-09-22 20:34:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][800/1251]	eta 0:09:04 lr 0.000100	time 1.2012 (1.2073)	loss 267.5310 (270.2117)	attn_loss 261.0490 (263.8151)	hidden_loss 6.4820 (6.3966)	grad_norm 95.8861 (91.4513)	mem 26450MB
[2021-09-22 20:35:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][810/1251]	eta 0:08:52 lr 0.000100	time 1.2055 (1.2072)	loss 259.6190 (270.1627)	attn_loss 253.6762 (263.7668)	hidden_loss 5.9428 (6.3959)	grad_norm 91.4301 (91.3653)	mem 26450MB
[2021-09-22 20:35:20 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][820/1251]	eta 0:08:40 lr 0.000100	time 1.2043 (1.2071)	loss 269.7378 (270.1459)	attn_loss 263.2027 (263.7500)	hidden_loss 6.5351 (6.3959)	grad_norm 86.3073 (91.2461)	mem 26450MB
[2021-09-22 20:35:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][830/1251]	eta 0:08:28 lr 0.000100	time 1.1968 (1.2069)	loss 258.8444 (270.0966)	attn_loss 252.2258 (263.7011)	hidden_loss 6.6186 (6.3955)	grad_norm 70.3853 (91.2785)	mem 26450MB
[2021-09-22 20:35:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][840/1251]	eta 0:08:16 lr 0.000100	time 1.2227 (1.2069)	loss 266.0422 (270.0444)	attn_loss 259.6987 (263.6500)	hidden_loss 6.3434 (6.3944)	grad_norm 106.8838 (91.4115)	mem 26450MB
[2021-09-22 20:35:56 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][850/1251]	eta 0:08:03 lr 0.000100	time 1.1845 (1.2068)	loss 269.9531 (270.0081)	attn_loss 263.5332 (263.6141)	hidden_loss 6.4199 (6.3940)	grad_norm 154.6466 (91.8757)	mem 26450MB
[2021-09-22 20:36:08 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][860/1251]	eta 0:07:51 lr 0.000100	time 1.2073 (1.2067)	loss 258.4155 (269.9304)	attn_loss 252.3141 (263.5369)	hidden_loss 6.1014 (6.3935)	grad_norm 65.6621 (92.0003)	mem 26450MB
[2021-09-22 20:36:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][870/1251]	eta 0:07:39 lr 0.000100	time 1.1787 (1.2066)	loss 268.8736 (269.8732)	attn_loss 262.6703 (263.4803)	hidden_loss 6.2032 (6.3928)	grad_norm 58.3916 (92.4308)	mem 26450MB
[2021-09-22 20:36:32 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][880/1251]	eta 0:07:27 lr 0.000100	time 1.2075 (1.2066)	loss 272.2574 (269.8228)	attn_loss 265.9792 (263.4300)	hidden_loss 6.2781 (6.3928)	grad_norm 48.4070 (92.2460)	mem 26450MB
[2021-09-22 20:36:44 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][890/1251]	eta 0:07:15 lr 0.000100	time 1.1921 (1.2065)	loss 262.3390 (269.7648)	attn_loss 256.3742 (263.3733)	hidden_loss 5.9648 (6.3915)	grad_norm 124.4529 (92.0883)	mem 26450MB
[2021-09-22 20:36:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][900/1251]	eta 0:07:03 lr 0.000100	time 1.1883 (1.2064)	loss 264.6472 (269.6975)	attn_loss 258.2562 (263.3073)	hidden_loss 6.3910 (6.3902)	grad_norm 92.7106 (92.2183)	mem 26450MB
[2021-09-22 20:37:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][910/1251]	eta 0:06:51 lr 0.000100	time 1.1988 (1.2063)	loss 257.0150 (269.6184)	attn_loss 250.9123 (263.2298)	hidden_loss 6.1026 (6.3886)	grad_norm 94.3013 (92.3462)	mem 26450MB
[2021-09-22 20:37:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][920/1251]	eta 0:06:39 lr 0.000100	time 1.1931 (1.2062)	loss 266.3889 (269.5951)	attn_loss 260.2218 (263.2066)	hidden_loss 6.1671 (6.3886)	grad_norm 148.6462 (92.8394)	mem 26450MB
[2021-09-22 20:37:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][930/1251]	eta 0:06:27 lr 0.000100	time 1.2119 (1.2061)	loss 258.5341 (269.5191)	attn_loss 252.3275 (263.1309)	hidden_loss 6.2067 (6.3882)	grad_norm 136.7868 (93.0590)	mem 26450MB
[2021-09-22 20:37:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][940/1251]	eta 0:06:15 lr 0.000100	time 1.2036 (1.2060)	loss 259.9268 (269.4542)	attn_loss 252.8589 (263.0665)	hidden_loss 7.0680 (6.3877)	grad_norm 111.3023 (93.4418)	mem 26450MB
[2021-09-22 20:37:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][950/1251]	eta 0:06:02 lr 0.000100	time 1.1947 (1.2060)	loss 258.3004 (269.3568)	attn_loss 251.7452 (262.9699)	hidden_loss 6.5553 (6.3868)	grad_norm 76.8705 (93.4822)	mem 26450MB
[2021-09-22 20:38:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][960/1251]	eta 0:05:50 lr 0.000100	time 1.1836 (1.2060)	loss 269.9522 (269.3075)	attn_loss 263.7221 (262.9205)	hidden_loss 6.2301 (6.3870)	grad_norm 157.1432 (93.8829)	mem 26450MB
[2021-09-22 20:38:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][970/1251]	eta 0:05:38 lr 0.000100	time 1.1836 (1.2059)	loss 259.5168 (269.2197)	attn_loss 253.2265 (262.8324)	hidden_loss 6.2903 (6.3873)	grad_norm 101.7586 (94.0211)	mem 26450MB
[2021-09-22 20:38:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][980/1251]	eta 0:05:26 lr 0.000100	time 1.1927 (1.2058)	loss 261.1410 (269.1690)	attn_loss 254.8236 (262.7814)	hidden_loss 6.3174 (6.3876)	grad_norm 90.3344 (93.9474)	mem 26450MB
[2021-09-22 20:38:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][990/1251]	eta 0:05:14 lr 0.000100	time 1.2002 (1.2058)	loss 260.2927 (269.1070)	attn_loss 253.6699 (262.7197)	hidden_loss 6.6228 (6.3872)	grad_norm 113.5538 (93.9834)	mem 26450MB
[2021-09-22 20:38:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1000/1251]	eta 0:05:02 lr 0.000100	time 1.1956 (1.2057)	loss 270.9958 (269.0414)	attn_loss 264.8110 (262.6540)	hidden_loss 6.1848 (6.3874)	grad_norm 84.1546 (94.0357)	mem 26450MB
[2021-09-22 20:39:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1010/1251]	eta 0:04:50 lr 0.000100	time 1.1991 (1.2056)	loss 260.2112 (268.9949)	attn_loss 253.6908 (262.6069)	hidden_loss 6.5204 (6.3880)	grad_norm 127.5197 (94.3621)	mem 26450MB
[2021-09-22 20:39:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1020/1251]	eta 0:04:38 lr 0.000100	time 1.1854 (1.2055)	loss 269.1933 (268.9661)	attn_loss 262.8291 (262.5776)	hidden_loss 6.3642 (6.3885)	grad_norm 157.1257 (94.5877)	mem 26450MB
[2021-09-22 20:39:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1030/1251]	eta 0:04:26 lr 0.000100	time 1.1886 (1.2055)	loss 265.8630 (268.9239)	attn_loss 259.4058 (262.5360)	hidden_loss 6.4572 (6.3879)	grad_norm 102.9971 (94.7556)	mem 26450MB
[2021-09-22 20:39:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1040/1251]	eta 0:04:14 lr 0.000100	time 1.1886 (1.2054)	loss 268.8849 (268.8914)	attn_loss 262.5339 (262.5047)	hidden_loss 6.3511 (6.3867)	grad_norm 121.6226 (94.8279)	mem 26450MB
[2021-09-22 20:39:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1050/1251]	eta 0:04:02 lr 0.000100	time 1.1880 (1.2054)	loss 261.5784 (268.8083)	attn_loss 255.2513 (262.4212)	hidden_loss 6.3271 (6.3871)	grad_norm 156.4419 (95.1774)	mem 26450MB
[2021-09-22 20:40:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1060/1251]	eta 0:03:50 lr 0.000100	time 1.1904 (1.2053)	loss 260.5654 (268.7505)	attn_loss 254.2500 (262.3646)	hidden_loss 6.3155 (6.3859)	grad_norm 124.3986 (95.4168)	mem 26450MB
[2021-09-22 20:40:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1070/1251]	eta 0:03:38 lr 0.000100	time 1.2027 (1.2053)	loss 262.0470 (268.7061)	attn_loss 255.7691 (262.3210)	hidden_loss 6.2779 (6.3850)	grad_norm 138.9541 (95.6393)	mem 26450MB
[2021-09-22 20:40:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1080/1251]	eta 0:03:26 lr 0.000100	time 1.1847 (1.2052)	loss 269.5020 (268.6664)	attn_loss 263.2677 (262.2828)	hidden_loss 6.2343 (6.3836)	grad_norm 103.0552 (95.8077)	mem 26450MB
[2021-09-22 20:40:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1090/1251]	eta 0:03:14 lr 0.000100	time 1.2035 (1.2052)	loss 259.3487 (268.6082)	attn_loss 252.8896 (262.2253)	hidden_loss 6.4591 (6.3829)	grad_norm 129.9827 (95.9026)	mem 26450MB
[2021-09-22 20:40:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1100/1251]	eta 0:03:01 lr 0.000100	time 1.2019 (1.2052)	loss 268.6066 (268.5603)	attn_loss 262.4460 (262.1778)	hidden_loss 6.1606 (6.3825)	grad_norm 92.7130 (95.9785)	mem 26450MB
[2021-09-22 20:41:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1110/1251]	eta 0:02:49 lr 0.000100	time 1.1945 (1.2051)	loss 269.2631 (268.5076)	attn_loss 262.8542 (262.1257)	hidden_loss 6.4089 (6.3820)	grad_norm 62.6222 (95.8562)	mem 26450MB
[2021-09-22 20:41:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1120/1251]	eta 0:02:37 lr 0.000100	time 1.1847 (1.2051)	loss 270.6590 (268.4664)	attn_loss 264.1450 (262.0838)	hidden_loss 6.5140 (6.3826)	grad_norm 72.3679 (95.6971)	mem 26450MB
[2021-09-22 20:41:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1130/1251]	eta 0:02:25 lr 0.000100	time 1.1974 (1.2050)	loss 257.2924 (268.4332)	attn_loss 251.0323 (262.0513)	hidden_loss 6.2601 (6.3819)	grad_norm 68.6166 (95.4557)	mem 26450MB
[2021-09-22 20:41:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1140/1251]	eta 0:02:13 lr 0.000100	time 1.2016 (1.2049)	loss 259.3232 (268.3737)	attn_loss 252.3242 (261.9909)	hidden_loss 6.9989 (6.3828)	grad_norm 103.2330 (95.3380)	mem 26450MB
[2021-09-22 20:41:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1150/1251]	eta 0:02:01 lr 0.000100	time 1.2042 (1.2049)	loss 262.7420 (268.3261)	attn_loss 256.5137 (261.9417)	hidden_loss 6.2283 (6.3843)	grad_norm 56.2751 (95.1927)	mem 26450MB
[2021-09-22 20:42:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1160/1251]	eta 0:01:49 lr 0.000100	time 1.2161 (1.2048)	loss 265.3396 (268.2870)	attn_loss 258.8865 (261.9020)	hidden_loss 6.4531 (6.3850)	grad_norm 80.7672 (95.0215)	mem 26450MB
[2021-09-22 20:42:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1170/1251]	eta 0:01:37 lr 0.000100	time 1.1963 (1.2048)	loss 264.0761 (268.2277)	attn_loss 257.4742 (261.8428)	hidden_loss 6.6019 (6.3850)	grad_norm 124.3903 (95.0428)	mem 26450MB
[2021-09-22 20:42:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1180/1251]	eta 0:01:25 lr 0.000100	time 1.2284 (1.2048)	loss 259.6399 (268.1956)	attn_loss 253.4123 (261.8110)	hidden_loss 6.2276 (6.3846)	grad_norm 121.4563 (95.2645)	mem 26450MB
[2021-09-22 20:42:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1190/1251]	eta 0:01:13 lr 0.000100	time 1.2132 (1.2047)	loss 256.4100 (268.1591)	attn_loss 250.0627 (261.7754)	hidden_loss 6.3474 (6.3838)	grad_norm 73.6321 (95.3393)	mem 26450MB
[2021-09-22 20:42:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1200/1251]	eta 0:01:01 lr 0.000100	time 1.1872 (1.2047)	loss 269.7115 (268.1326)	attn_loss 263.2087 (261.7487)	hidden_loss 6.5028 (6.3839)	grad_norm 95.6469 (95.5085)	mem 26450MB
[2021-09-22 20:43:07 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1210/1251]	eta 0:00:49 lr 0.000100	time 1.1912 (1.2046)	loss 258.6238 (268.0776)	attn_loss 252.3152 (261.6940)	hidden_loss 6.3087 (6.3836)	grad_norm 81.5508 (95.5175)	mem 26450MB
[2021-09-22 20:43:19 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1220/1251]	eta 0:00:37 lr 0.000100	time 1.2076 (1.2045)	loss 256.4865 (268.0371)	attn_loss 250.4352 (261.6540)	hidden_loss 6.0514 (6.3832)	grad_norm 70.0968 (95.3822)	mem 26450MB
[2021-09-22 20:43:31 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1230/1251]	eta 0:00:25 lr 0.000100	time 1.2015 (1.2045)	loss 265.2039 (267.9933)	attn_loss 258.9705 (261.6111)	hidden_loss 6.2334 (6.3822)	grad_norm 85.6512 (95.2173)	mem 26450MB
[2021-09-22 20:43:43 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1240/1251]	eta 0:00:13 lr 0.000100	time 1.1916 (1.2045)	loss 257.1846 (267.9565)	attn_loss 250.7468 (261.5742)	hidden_loss 6.4378 (6.3823)	grad_norm 126.9319 (95.1607)	mem 26450MB
[2021-09-22 20:43:55 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [1/3][1250/1251]	eta 0:00:01 lr 0.000100	time 1.1959 (1.2044)	loss 258.1335 (267.8911)	attn_loss 251.8903 (261.5092)	hidden_loss 6.2432 (6.3819)	grad_norm 129.1178 (95.2701)	mem 26450MB
[2021-09-22 20:43:56 swin_tiny_patch4_window7_224] (main.py 371): INFO EPOCH 1 training takes 0:25:07
[2021-09-22 20:43:56 swin_tiny_patch4_window7_224] (utils.py 63): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_1.pth saving......
[2021-09-22 20:43:57 swin_tiny_patch4_window7_224] (utils.py 65): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_1.pth saved !!!
[2021-09-22 20:44:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][0/1251]	eta 2:39:40 lr 0.000100	time 7.6586 (7.6586)	loss 266.7083 (266.7083)	attn_loss 260.4348 (260.4348)	hidden_loss 6.2736 (6.2736)	grad_norm 92.0923 (92.0923)	mem 26450MB
[2021-09-22 20:44:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][10/1251]	eta 0:37:08 lr 0.000100	time 1.1936 (1.7957)	loss 256.7435 (264.8521)	attn_loss 250.3537 (258.4811)	hidden_loss 6.3898 (6.3710)	grad_norm 74.8222 (105.7922)	mem 26450MB
[2021-09-22 20:44:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][20/1251]	eta 0:30:59 lr 0.000100	time 1.1868 (1.5104)	loss 262.3224 (263.0918)	attn_loss 256.1057 (256.7131)	hidden_loss 6.2168 (6.3787)	grad_norm 119.0055 (111.1473)	mem 26450MB
[2021-09-22 20:44:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][30/1251]	eta 0:28:41 lr 0.000100	time 1.1941 (1.4103)	loss 265.9016 (262.8360)	attn_loss 259.6179 (256.4624)	hidden_loss 6.2838 (6.3737)	grad_norm 64.0547 (104.2365)	mem 26450MB
[2021-09-22 20:44:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][40/1251]	eta 0:27:25 lr 0.000100	time 1.2218 (1.3592)	loss 269.6352 (263.6491)	attn_loss 263.0273 (257.2669)	hidden_loss 6.6080 (6.3823)	grad_norm 119.9570 (97.8285)	mem 26450MB
[2021-09-22 20:45:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][50/1251]	eta 0:26:34 lr 0.000100	time 1.1922 (1.3278)	loss 257.0414 (263.2447)	attn_loss 250.7209 (256.8681)	hidden_loss 6.3204 (6.3766)	grad_norm 92.0775 (99.7427)	mem 26450MB
[2021-09-22 20:45:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][60/1251]	eta 0:25:56 lr 0.000100	time 1.2122 (1.3071)	loss 257.6009 (263.1947)	attn_loss 251.0096 (256.8232)	hidden_loss 6.5913 (6.3715)	grad_norm 74.5120 (100.1777)	mem 26450MB
[2021-09-22 20:45:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][70/1251]	eta 0:25:26 lr 0.000100	time 1.2092 (1.2926)	loss 268.5503 (263.0429)	attn_loss 262.3417 (256.6772)	hidden_loss 6.2086 (6.3658)	grad_norm 62.3995 (100.0468)	mem 26450MB
[2021-09-22 20:45:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][80/1251]	eta 0:24:59 lr 0.000100	time 1.1975 (1.2807)	loss 257.5076 (262.4151)	attn_loss 251.2318 (256.0639)	hidden_loss 6.2758 (6.3513)	grad_norm 110.0820 (98.3020)	mem 26450MB
[2021-09-22 20:45:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][90/1251]	eta 0:24:36 lr 0.000100	time 1.1826 (1.2716)	loss 268.6704 (262.3826)	attn_loss 262.5093 (256.0341)	hidden_loss 6.1610 (6.3485)	grad_norm 84.3041 (96.2578)	mem 26450MB
[2021-09-22 20:46:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][100/1251]	eta 0:24:15 lr 0.000100	time 1.1935 (1.2642)	loss 267.8204 (262.4948)	attn_loss 261.5000 (256.1491)	hidden_loss 6.3203 (6.3456)	grad_norm 84.4224 (94.8931)	mem 26450MB
[2021-09-22 20:46:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][110/1251]	eta 0:23:55 lr 0.000100	time 1.1969 (1.2583)	loss 256.6260 (262.4009)	attn_loss 250.3267 (256.0716)	hidden_loss 6.2993 (6.3293)	grad_norm 62.3477 (93.0559)	mem 26450MB
[2021-09-22 20:46:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][120/1251]	eta 0:23:37 lr 0.000100	time 1.2183 (1.2536)	loss 255.1962 (262.2015)	attn_loss 249.1069 (255.8717)	hidden_loss 6.0893 (6.3299)	grad_norm 77.7629 (92.3633)	mem 26450MB
[2021-09-22 20:46:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][130/1251]	eta 0:23:20 lr 0.000100	time 1.2003 (1.2496)	loss 269.5101 (262.0537)	attn_loss 263.4329 (255.7306)	hidden_loss 6.0772 (6.3231)	grad_norm 182.9629 (93.2602)	mem 26450MB
[2021-09-22 20:46:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][140/1251]	eta 0:23:04 lr 0.000100	time 1.1983 (1.2460)	loss 257.3685 (262.0888)	attn_loss 251.0440 (255.7615)	hidden_loss 6.3244 (6.3272)	grad_norm 55.1478 (94.1437)	mem 26450MB
[2021-09-22 20:47:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][150/1251]	eta 0:22:48 lr 0.000100	time 1.1862 (1.2431)	loss 261.2133 (262.0829)	attn_loss 254.8485 (255.7607)	hidden_loss 6.3648 (6.3222)	grad_norm 65.6261 (93.2204)	mem 26450MB
[2021-09-22 20:47:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][160/1251]	eta 0:22:33 lr 0.000100	time 1.1960 (1.2404)	loss 267.8807 (262.0038)	attn_loss 261.7486 (255.6848)	hidden_loss 6.1321 (6.3189)	grad_norm 80.6678 (92.3621)	mem 26450MB
[2021-09-22 20:47:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][170/1251]	eta 0:22:18 lr 0.000100	time 1.2153 (1.2380)	loss 256.4688 (261.8565)	attn_loss 250.1231 (255.5353)	hidden_loss 6.3457 (6.3212)	grad_norm 71.8052 (92.4720)	mem 26450MB
[2021-09-22 20:47:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][180/1251]	eta 0:22:03 lr 0.000100	time 1.1965 (1.2358)	loss 258.4675 (261.9084)	attn_loss 252.1617 (255.5882)	hidden_loss 6.3059 (6.3202)	grad_norm 100.4444 (91.9411)	mem 26450MB
[2021-09-22 20:47:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][190/1251]	eta 0:21:49 lr 0.000100	time 1.1848 (1.2340)	loss 262.0873 (261.8885)	attn_loss 255.4902 (255.5714)	hidden_loss 6.5971 (6.3171)	grad_norm 86.1870 (91.9339)	mem 26450MB
[2021-09-22 20:48:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][200/1251]	eta 0:21:35 lr 0.000100	time 1.2016 (1.2324)	loss 258.6939 (261.8188)	attn_loss 252.4333 (255.4985)	hidden_loss 6.2606 (6.3203)	grad_norm 99.1730 (91.1535)	mem 26450MB
[2021-09-22 20:48:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][210/1251]	eta 0:21:21 lr 0.000100	time 1.2062 (1.2307)	loss 266.8249 (261.8921)	attn_loss 260.4086 (255.5731)	hidden_loss 6.4162 (6.3190)	grad_norm 63.2430 (90.1019)	mem 26450MB
[2021-09-22 20:48:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][220/1251]	eta 0:21:07 lr 0.000100	time 1.1924 (1.2296)	loss 262.4827 (261.8498)	attn_loss 256.0652 (255.5242)	hidden_loss 6.4175 (6.3256)	grad_norm 120.8709 (89.4209)	mem 26450MB
[2021-09-22 20:48:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][230/1251]	eta 0:20:54 lr 0.000100	time 1.1874 (1.2285)	loss 267.0655 (261.9769)	attn_loss 260.7523 (255.6523)	hidden_loss 6.3132 (6.3246)	grad_norm 100.9765 (90.6914)	mem 26450MB
[2021-09-22 20:48:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][240/1251]	eta 0:20:40 lr 0.000100	time 1.2267 (1.2273)	loss 256.8365 (261.8376)	attn_loss 250.5283 (255.5172)	hidden_loss 6.3082 (6.3205)	grad_norm 75.7791 (91.8385)	mem 26450MB
[2021-09-22 20:49:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][250/1251]	eta 0:20:27 lr 0.000100	time 1.1845 (1.2263)	loss 253.7488 (261.7660)	attn_loss 247.8632 (255.4491)	hidden_loss 5.8857 (6.3168)	grad_norm 93.2813 (92.5089)	mem 26450MB
[2021-09-22 20:49:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][260/1251]	eta 0:20:14 lr 0.000100	time 1.1807 (1.2253)	loss 268.2421 (261.7687)	attn_loss 262.0003 (255.4549)	hidden_loss 6.2418 (6.3138)	grad_norm 118.7156 (91.9735)	mem 26450MB
[2021-09-22 20:49:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][270/1251]	eta 0:20:01 lr 0.000100	time 1.2068 (1.2243)	loss 268.3346 (261.8057)	attn_loss 261.8894 (255.4927)	hidden_loss 6.4452 (6.3130)	grad_norm 81.7434 (92.0923)	mem 26450MB
[2021-09-22 20:49:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][280/1251]	eta 0:19:48 lr 0.000100	time 1.1943 (1.2235)	loss 268.6171 (261.7893)	attn_loss 262.2784 (255.4767)	hidden_loss 6.3386 (6.3126)	grad_norm 81.2217 (91.3509)	mem 26450MB
[2021-09-22 20:49:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][290/1251]	eta 0:19:34 lr 0.000100	time 1.1989 (1.2226)	loss 254.3231 (261.7141)	attn_loss 248.3598 (255.4047)	hidden_loss 5.9633 (6.3094)	grad_norm 116.6563 (91.7276)	mem 26450MB
[2021-09-22 20:50:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][300/1251]	eta 0:19:21 lr 0.000100	time 1.1983 (1.2217)	loss 258.0614 (261.6420)	attn_loss 251.6706 (255.3297)	hidden_loss 6.3908 (6.3123)	grad_norm 134.0075 (92.8039)	mem 26450MB
[2021-09-22 20:50:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][310/1251]	eta 0:19:08 lr 0.000100	time 1.2003 (1.2209)	loss 259.5623 (261.6176)	attn_loss 253.2323 (255.3086)	hidden_loss 6.3301 (6.3089)	grad_norm 85.1749 (93.2573)	mem 26450MB
[2021-09-22 20:50:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][320/1251]	eta 0:18:56 lr 0.000100	time 1.2027 (1.2205)	loss 258.7686 (261.6626)	attn_loss 252.7670 (255.3545)	hidden_loss 6.0017 (6.3081)	grad_norm 71.9922 (93.5088)	mem 26450MB
[2021-09-22 20:50:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][330/1251]	eta 0:18:43 lr 0.000100	time 1.1909 (1.2197)	loss 267.9021 (261.6883)	attn_loss 261.5222 (255.3791)	hidden_loss 6.3799 (6.3092)	grad_norm 109.9359 (93.3295)	mem 26450MB
[2021-09-22 20:50:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][340/1251]	eta 0:18:30 lr 0.000100	time 1.1923 (1.2191)	loss 254.9172 (261.6536)	attn_loss 248.8439 (255.3483)	hidden_loss 6.0733 (6.3053)	grad_norm 105.6698 (93.0578)	mem 26450MB
[2021-09-22 20:51:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][350/1251]	eta 0:18:17 lr 0.000100	time 1.1949 (1.2186)	loss 255.3607 (261.5860)	attn_loss 249.2476 (255.2752)	hidden_loss 6.1131 (6.3108)	grad_norm 65.3169 (92.5172)	mem 26450MB
[2021-09-22 20:51:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][360/1251]	eta 0:18:05 lr 0.000100	time 1.1943 (1.2179)	loss 262.1707 (261.5078)	attn_loss 255.8590 (255.1994)	hidden_loss 6.3117 (6.3084)	grad_norm 103.9854 (92.2199)	mem 26450MB
[2021-09-22 20:51:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][370/1251]	eta 0:17:52 lr 0.000100	time 1.1871 (1.2175)	loss 258.7672 (261.5185)	attn_loss 252.4918 (255.2080)	hidden_loss 6.2754 (6.3104)	grad_norm 109.7126 (92.1145)	mem 26450MB
[2021-09-22 20:51:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][380/1251]	eta 0:17:39 lr 0.000100	time 1.2050 (1.2170)	loss 266.6831 (261.4924)	attn_loss 260.5783 (255.1826)	hidden_loss 6.1048 (6.3098)	grad_norm 76.3817 (91.7914)	mem 26450MB
[2021-09-22 20:51:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][390/1251]	eta 0:17:27 lr 0.000100	time 1.2117 (1.2164)	loss 255.9628 (261.4708)	attn_loss 249.7362 (255.1583)	hidden_loss 6.2266 (6.3126)	grad_norm 85.2865 (91.3283)	mem 26450MB
[2021-09-22 20:52:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][400/1251]	eta 0:17:14 lr 0.000100	time 1.2088 (1.2160)	loss 255.0027 (261.4683)	attn_loss 248.8940 (255.1560)	hidden_loss 6.1087 (6.3123)	grad_norm 120.7123 (91.1865)	mem 26450MB
[2021-09-22 20:52:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][410/1251]	eta 0:17:02 lr 0.000100	time 1.2012 (1.2155)	loss 267.9014 (261.4727)	attn_loss 261.6417 (255.1621)	hidden_loss 6.2596 (6.3106)	grad_norm 89.3612 (91.4452)	mem 26450MB
[2021-09-22 20:52:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][420/1251]	eta 0:16:49 lr 0.000100	time 1.1974 (1.2151)	loss 265.7673 (261.4514)	attn_loss 259.7372 (255.1413)	hidden_loss 6.0301 (6.3101)	grad_norm 118.0544 (91.4125)	mem 26450MB
[2021-09-22 20:52:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][430/1251]	eta 0:16:37 lr 0.000100	time 1.2015 (1.2149)	loss 267.9303 (261.5131)	attn_loss 261.5275 (255.2030)	hidden_loss 6.4029 (6.3101)	grad_norm 128.3260 (92.2523)	mem 26450MB
[2021-09-22 20:52:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][440/1251]	eta 0:16:24 lr 0.000100	time 1.2072 (1.2145)	loss 260.6114 (261.5463)	attn_loss 254.2706 (255.2359)	hidden_loss 6.3408 (6.3104)	grad_norm 96.7773 (92.7784)	mem 26450MB
[2021-09-22 20:53:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][450/1251]	eta 0:16:12 lr 0.000100	time 1.1859 (1.2141)	loss 266.3554 (261.4943)	attn_loss 260.0425 (255.1846)	hidden_loss 6.3129 (6.3097)	grad_norm 52.9401 (92.6091)	mem 26450MB
[2021-09-22 20:53:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][460/1251]	eta 0:16:00 lr 0.000100	time 1.1929 (1.2139)	loss 254.9847 (261.4772)	attn_loss 248.0889 (255.1666)	hidden_loss 6.8958 (6.3106)	grad_norm 69.2071 (92.8200)	mem 26450MB
[2021-09-22 20:53:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][470/1251]	eta 0:15:47 lr 0.000100	time 1.1954 (1.2136)	loss 263.6695 (261.4747)	attn_loss 257.4926 (255.1654)	hidden_loss 6.1770 (6.3093)	grad_norm 94.9883 (92.9382)	mem 26450MB
[2021-09-22 20:53:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][480/1251]	eta 0:15:35 lr 0.000100	time 1.2017 (1.2132)	loss 263.5785 (261.4209)	attn_loss 257.4288 (255.1128)	hidden_loss 6.1497 (6.3082)	grad_norm 132.8629 (93.1690)	mem 26450MB
[2021-09-22 20:53:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][490/1251]	eta 0:15:23 lr 0.000100	time 1.1988 (1.2129)	loss 265.4493 (261.4128)	attn_loss 259.3657 (255.1061)	hidden_loss 6.0836 (6.3066)	grad_norm 64.5198 (93.1017)	mem 26450MB
[2021-09-22 20:54:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][500/1251]	eta 0:15:10 lr 0.000100	time 1.1883 (1.2126)	loss 256.6179 (261.4169)	attn_loss 250.3754 (255.1115)	hidden_loss 6.2425 (6.3054)	grad_norm 83.1780 (92.8546)	mem 26450MB
[2021-09-22 20:54:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][510/1251]	eta 0:14:58 lr 0.000100	time 1.2313 (1.2124)	loss 259.0713 (261.4054)	attn_loss 253.0447 (255.1023)	hidden_loss 6.0266 (6.3031)	grad_norm 77.2504 (92.8378)	mem 26450MB
[2021-09-22 20:54:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][520/1251]	eta 0:14:46 lr 0.000100	time 1.1999 (1.2122)	loss 256.6483 (261.3947)	attn_loss 250.1182 (255.0933)	hidden_loss 6.5301 (6.3014)	grad_norm 88.5706 (92.6470)	mem 26450MB
[2021-09-22 20:54:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][530/1251]	eta 0:14:33 lr 0.000100	time 1.1872 (1.2119)	loss 267.1413 (261.3917)	attn_loss 260.9151 (255.0897)	hidden_loss 6.2262 (6.3020)	grad_norm 76.3061 (92.2037)	mem 26450MB
[2021-09-22 20:54:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][540/1251]	eta 0:14:21 lr 0.000100	time 1.1927 (1.2116)	loss 256.3665 (261.3600)	attn_loss 250.3232 (255.0613)	hidden_loss 6.0434 (6.2987)	grad_norm 87.4221 (92.2827)	mem 26450MB
[2021-09-22 20:55:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][550/1251]	eta 0:14:09 lr 0.000100	time 1.2193 (1.2114)	loss 254.2836 (261.3061)	attn_loss 248.2952 (255.0065)	hidden_loss 5.9885 (6.2996)	grad_norm 106.3271 (92.0094)	mem 26450MB
[2021-09-22 20:55:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][560/1251]	eta 0:13:57 lr 0.000100	time 1.2069 (1.2113)	loss 262.6594 (261.2874)	attn_loss 256.4741 (254.9894)	hidden_loss 6.1854 (6.2980)	grad_norm 62.0257 (91.8109)	mem 26450MB
[2021-09-22 20:55:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][570/1251]	eta 0:13:44 lr 0.000100	time 1.1862 (1.2111)	loss 257.6326 (261.2645)	attn_loss 251.7184 (254.9667)	hidden_loss 5.9142 (6.2978)	grad_norm 118.3531 (91.9172)	mem 26450MB
[2021-09-22 20:55:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][580/1251]	eta 0:13:32 lr 0.000100	time 1.1993 (1.2110)	loss 258.0232 (261.2010)	attn_loss 252.1844 (254.9067)	hidden_loss 5.8388 (6.2943)	grad_norm 131.5405 (92.3169)	mem 26450MB
[2021-09-22 20:55:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][590/1251]	eta 0:13:20 lr 0.000100	time 1.2248 (1.2109)	loss 256.5485 (261.1655)	attn_loss 250.4450 (254.8712)	hidden_loss 6.1035 (6.2943)	grad_norm 89.3148 (92.3096)	mem 26450MB
[2021-09-22 20:56:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][600/1251]	eta 0:13:08 lr 0.000100	time 1.2059 (1.2109)	loss 253.6429 (261.1299)	attn_loss 246.8517 (254.8345)	hidden_loss 6.7912 (6.2953)	grad_norm 80.9457 (92.2798)	mem 26450MB
[2021-09-22 20:56:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][610/1251]	eta 0:12:56 lr 0.000100	time 1.1891 (1.2107)	loss 252.3565 (261.0714)	attn_loss 246.1606 (254.7778)	hidden_loss 6.1960 (6.2936)	grad_norm 109.7685 (92.0526)	mem 26450MB
[2021-09-22 20:56:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][620/1251]	eta 0:12:43 lr 0.000100	time 1.1905 (1.2105)	loss 266.6335 (261.0665)	attn_loss 260.2864 (254.7746)	hidden_loss 6.3471 (6.2919)	grad_norm 74.3204 (91.7614)	mem 26450MB
[2021-09-22 20:56:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][630/1251]	eta 0:12:31 lr 0.000100	time 1.2121 (1.2104)	loss 266.1779 (261.0551)	attn_loss 259.8712 (254.7644)	hidden_loss 6.3067 (6.2907)	grad_norm 56.1262 (91.5727)	mem 26450MB
[2021-09-22 20:56:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][640/1251]	eta 0:12:19 lr 0.000100	time 1.2088 (1.2102)	loss 268.2933 (261.0161)	attn_loss 262.0773 (254.7247)	hidden_loss 6.2160 (6.2914)	grad_norm 70.8597 (91.2919)	mem 26450MB
[2021-09-22 20:57:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][650/1251]	eta 0:12:07 lr 0.000100	time 1.1873 (1.2101)	loss 256.5962 (261.0287)	attn_loss 250.4565 (254.7386)	hidden_loss 6.1397 (6.2901)	grad_norm 60.9024 (90.9957)	mem 26450MB
[2021-09-22 20:57:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][660/1251]	eta 0:11:55 lr 0.000100	time 1.2008 (1.2099)	loss 265.2065 (261.0241)	attn_loss 258.9995 (254.7362)	hidden_loss 6.2070 (6.2878)	grad_norm 97.5505 (90.7686)	mem 26450MB
[2021-09-22 20:57:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][670/1251]	eta 0:11:42 lr 0.000100	time 1.1946 (1.2098)	loss 268.0078 (261.0060)	attn_loss 261.6733 (254.7202)	hidden_loss 6.3344 (6.2858)	grad_norm 91.2029 (90.7042)	mem 26450MB
[2021-09-22 20:57:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][680/1251]	eta 0:11:30 lr 0.000100	time 1.1871 (1.2097)	loss 254.6500 (260.9606)	attn_loss 247.7707 (254.6748)	hidden_loss 6.8793 (6.2859)	grad_norm 95.7651 (90.9905)	mem 26450MB
[2021-09-22 20:57:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][690/1251]	eta 0:11:18 lr 0.000100	time 1.2123 (1.2096)	loss 267.4283 (260.9416)	attn_loss 261.1221 (254.6573)	hidden_loss 6.3062 (6.2843)	grad_norm 86.8037 (91.0779)	mem 26450MB
[2021-09-22 20:58:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][700/1251]	eta 0:11:06 lr 0.000100	time 1.1984 (1.2095)	loss 267.5518 (260.9517)	attn_loss 261.1381 (254.6663)	hidden_loss 6.4137 (6.2853)	grad_norm 87.7888 (90.9466)	mem 26450MB
[2021-09-22 20:58:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][710/1251]	eta 0:10:54 lr 0.000100	time 1.2034 (1.2093)	loss 266.5163 (260.9752)	attn_loss 260.1209 (254.6890)	hidden_loss 6.3954 (6.2861)	grad_norm 59.3983 (90.7247)	mem 26450MB
[2021-09-22 20:58:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][720/1251]	eta 0:10:42 lr 0.000100	time 1.1872 (1.2093)	loss 263.9820 (260.9750)	attn_loss 257.8326 (254.6889)	hidden_loss 6.1494 (6.2861)	grad_norm 117.1032 (90.9745)	mem 26450MB
[2021-09-22 20:58:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][730/1251]	eta 0:10:29 lr 0.000100	time 1.1981 (1.2092)	loss 254.1688 (260.9796)	attn_loss 247.7247 (254.6934)	hidden_loss 6.4441 (6.2863)	grad_norm 109.4587 (91.1300)	mem 26450MB
[2021-09-22 20:58:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][740/1251]	eta 0:10:17 lr 0.000100	time 1.1931 (1.2090)	loss 263.3745 (260.9748)	attn_loss 257.1522 (254.6905)	hidden_loss 6.2222 (6.2844)	grad_norm 87.0867 (91.0262)	mem 26450MB
[2021-09-22 20:59:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][750/1251]	eta 0:10:05 lr 0.000100	time 1.1847 (1.2089)	loss 258.2463 (261.0016)	attn_loss 251.6322 (254.7180)	hidden_loss 6.6142 (6.2836)	grad_norm 58.8889 (90.7104)	mem 26450MB
[2021-09-22 20:59:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][760/1251]	eta 0:09:53 lr 0.000100	time 1.1952 (1.2089)	loss 266.7470 (261.0108)	attn_loss 260.4464 (254.7264)	hidden_loss 6.3006 (6.2844)	grad_norm 66.9461 (90.5863)	mem 26450MB
[2021-09-22 20:59:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][770/1251]	eta 0:09:41 lr 0.000100	time 1.1927 (1.2088)	loss 255.6655 (261.0047)	attn_loss 249.2307 (254.7196)	hidden_loss 6.4347 (6.2851)	grad_norm 70.0975 (90.4144)	mem 26450MB
[2021-09-22 20:59:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][780/1251]	eta 0:09:29 lr 0.000100	time 1.1982 (1.2088)	loss 265.6082 (260.9860)	attn_loss 259.4039 (254.7017)	hidden_loss 6.2043 (6.2843)	grad_norm 67.4544 (90.1058)	mem 26450MB
[2021-09-22 20:59:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][790/1251]	eta 0:09:17 lr 0.000100	time 1.2155 (1.2087)	loss 255.5642 (261.0060)	attn_loss 249.6202 (254.7225)	hidden_loss 5.9440 (6.2835)	grad_norm 80.9767 (89.8837)	mem 26450MB
[2021-09-22 21:00:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][800/1251]	eta 0:09:05 lr 0.000100	time 1.1992 (1.2085)	loss 268.0909 (261.0018)	attn_loss 261.8782 (254.7189)	hidden_loss 6.2126 (6.2829)	grad_norm 71.8887 (89.8871)	mem 26450MB
[2021-09-22 21:00:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][810/1251]	eta 0:08:52 lr 0.000100	time 1.1839 (1.2084)	loss 254.9135 (260.9922)	attn_loss 249.0975 (254.7106)	hidden_loss 5.8160 (6.2816)	grad_norm 83.7987 (89.9620)	mem 26450MB
[2021-09-22 21:00:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][820/1251]	eta 0:08:40 lr 0.000100	time 1.2320 (1.2083)	loss 255.6544 (260.9962)	attn_loss 249.6419 (254.7153)	hidden_loss 6.0125 (6.2809)	grad_norm 76.5472 (89.8829)	mem 26450MB
[2021-09-22 21:00:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][830/1251]	eta 0:08:28 lr 0.000100	time 1.1947 (1.2083)	loss 255.1437 (260.9677)	attn_loss 248.5469 (254.6863)	hidden_loss 6.5967 (6.2813)	grad_norm 85.7559 (89.8230)	mem 26450MB
[2021-09-22 21:00:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][840/1251]	eta 0:08:16 lr 0.000100	time 1.1907 (1.2082)	loss 254.8830 (260.9339)	attn_loss 248.9113 (254.6526)	hidden_loss 5.9716 (6.2813)	grad_norm 109.1669 (89.7598)	mem 26450MB
[2021-09-22 21:01:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][850/1251]	eta 0:08:04 lr 0.000100	time 1.2083 (1.2081)	loss 265.3439 (260.9316)	attn_loss 259.2486 (254.6511)	hidden_loss 6.0952 (6.2805)	grad_norm 83.4624 (89.8034)	mem 26450MB
[2021-09-22 21:01:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][860/1251]	eta 0:07:52 lr 0.000100	time 1.1930 (1.2080)	loss 256.2868 (260.9298)	attn_loss 250.0124 (254.6500)	hidden_loss 6.2743 (6.2798)	grad_norm 86.7089 (89.7429)	mem 26450MB
[2021-09-22 21:01:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][870/1251]	eta 0:07:40 lr 0.000100	time 1.1970 (1.2079)	loss 264.7740 (260.9241)	attn_loss 258.5187 (254.6449)	hidden_loss 6.2553 (6.2792)	grad_norm 61.6301 (89.7132)	mem 26450MB
[2021-09-22 21:01:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][880/1251]	eta 0:07:28 lr 0.000100	time 1.2067 (1.2079)	loss 265.4362 (260.9408)	attn_loss 259.1086 (254.6619)	hidden_loss 6.3275 (6.2789)	grad_norm 129.9986 (89.8489)	mem 26450MB
[2021-09-22 21:01:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][890/1251]	eta 0:07:16 lr 0.000100	time 1.2130 (1.2078)	loss 265.6599 (260.9404)	attn_loss 259.2038 (254.6619)	hidden_loss 6.4561 (6.2785)	grad_norm 71.3562 (89.8063)	mem 26450MB
[2021-09-22 21:02:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][900/1251]	eta 0:07:03 lr 0.000100	time 1.2006 (1.2077)	loss 253.8168 (260.8959)	attn_loss 247.7717 (254.6190)	hidden_loss 6.0451 (6.2769)	grad_norm 79.1738 (89.7780)	mem 26450MB
[2021-09-22 21:02:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][910/1251]	eta 0:06:51 lr 0.000100	time 1.1976 (1.2077)	loss 263.5566 (260.8800)	attn_loss 257.3680 (254.6031)	hidden_loss 6.1886 (6.2769)	grad_norm 122.7975 (90.0944)	mem 26450MB
[2021-09-22 21:02:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][920/1251]	eta 0:06:39 lr 0.000100	time 1.1895 (1.2076)	loss 258.6938 (260.8441)	attn_loss 252.6219 (254.5675)	hidden_loss 6.0720 (6.2765)	grad_norm 108.9729 (90.3393)	mem 26450MB
[2021-09-22 21:02:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][930/1251]	eta 0:06:27 lr 0.000100	time 1.2026 (1.2075)	loss 255.5575 (260.8266)	attn_loss 249.2485 (254.5509)	hidden_loss 6.3090 (6.2757)	grad_norm 75.7889 (90.5023)	mem 26450MB
[2021-09-22 21:02:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][940/1251]	eta 0:06:15 lr 0.000100	time 1.2191 (1.2074)	loss 263.0552 (260.8217)	attn_loss 256.5555 (254.5451)	hidden_loss 6.4997 (6.2766)	grad_norm 110.6126 (90.6731)	mem 26450MB
[2021-09-22 21:03:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][950/1251]	eta 0:06:03 lr 0.000100	time 1.2044 (1.2074)	loss 266.8275 (260.7935)	attn_loss 260.5529 (254.5169)	hidden_loss 6.2746 (6.2766)	grad_norm 104.2674 (90.7364)	mem 26450MB
[2021-09-22 21:03:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][960/1251]	eta 0:05:51 lr 0.000100	time 1.2017 (1.2073)	loss 268.6164 (260.7927)	attn_loss 262.3896 (254.5158)	hidden_loss 6.2268 (6.2768)	grad_norm 136.1914 (90.8302)	mem 26450MB
[2021-09-22 21:03:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][970/1251]	eta 0:05:39 lr 0.000100	time 1.2034 (1.2072)	loss 255.8411 (260.7544)	attn_loss 249.0490 (254.4763)	hidden_loss 6.7921 (6.2781)	grad_norm 119.3908 (90.7869)	mem 26450MB
[2021-09-22 21:03:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][980/1251]	eta 0:05:27 lr 0.000100	time 1.1992 (1.2071)	loss 258.0104 (260.7595)	attn_loss 251.6032 (254.4816)	hidden_loss 6.4071 (6.2778)	grad_norm 99.9220 (90.7699)	mem 26450MB
[2021-09-22 21:03:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][990/1251]	eta 0:05:15 lr 0.000100	time 1.2279 (1.2071)	loss 264.6139 (260.7767)	attn_loss 258.3929 (254.4993)	hidden_loss 6.2210 (6.2773)	grad_norm 55.6376 (90.6566)	mem 26450MB
[2021-09-22 21:04:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1000/1251]	eta 0:05:02 lr 0.000100	time 1.2061 (1.2070)	loss 265.9419 (260.7530)	attn_loss 259.7850 (254.4769)	hidden_loss 6.1568 (6.2760)	grad_norm 82.6632 (90.5400)	mem 26450MB
[2021-09-22 21:04:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1010/1251]	eta 0:04:50 lr 0.000100	time 1.1940 (1.2069)	loss 266.2745 (260.7454)	attn_loss 259.9060 (254.4698)	hidden_loss 6.3684 (6.2756)	grad_norm 86.1195 (90.3839)	mem 26450MB
[2021-09-22 21:04:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1020/1251]	eta 0:04:38 lr 0.000100	time 1.1968 (1.2068)	loss 266.9562 (260.7332)	attn_loss 260.6266 (254.4581)	hidden_loss 6.3296 (6.2751)	grad_norm 79.9904 (90.2506)	mem 26450MB
[2021-09-22 21:04:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1030/1251]	eta 0:04:26 lr 0.000100	time 1.1867 (1.2067)	loss 265.3107 (260.7202)	attn_loss 259.0923 (254.4456)	hidden_loss 6.2184 (6.2746)	grad_norm 127.5675 (90.1130)	mem 26450MB
[2021-09-22 21:04:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1040/1251]	eta 0:04:14 lr 0.000100	time 1.2109 (1.2067)	loss 257.4638 (260.7193)	attn_loss 251.3508 (254.4451)	hidden_loss 6.1130 (6.2743)	grad_norm 106.2035 (90.2149)	mem 26450MB
[2021-09-22 21:05:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1050/1251]	eta 0:04:02 lr 0.000100	time 1.1969 (1.2066)	loss 267.4895 (260.7166)	attn_loss 261.1154 (254.4424)	hidden_loss 6.3741 (6.2741)	grad_norm 67.9510 (90.0507)	mem 26450MB
[2021-09-22 21:05:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1060/1251]	eta 0:03:50 lr 0.000100	time 1.2051 (1.2065)	loss 266.0914 (260.7193)	attn_loss 259.9216 (254.4457)	hidden_loss 6.1698 (6.2736)	grad_norm 57.6444 (89.8363)	mem 26450MB
[2021-09-22 21:05:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1070/1251]	eta 0:03:38 lr 0.000100	time 1.1866 (1.2064)	loss 255.1540 (260.7011)	attn_loss 249.0859 (254.4281)	hidden_loss 6.0681 (6.2730)	grad_norm 78.8227 (89.9091)	mem 26450MB
[2021-09-22 21:05:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1080/1251]	eta 0:03:26 lr 0.000100	time 1.2071 (1.2064)	loss 262.8757 (260.7032)	attn_loss 256.6262 (254.4305)	hidden_loss 6.2495 (6.2726)	grad_norm 80.8421 (89.9960)	mem 26450MB
[2021-09-22 21:05:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1090/1251]	eta 0:03:14 lr 0.000100	time 1.1960 (1.2063)	loss 257.9707 (260.6883)	attn_loss 251.7326 (254.4167)	hidden_loss 6.2381 (6.2717)	grad_norm 102.4160 (89.9484)	mem 26450MB
[2021-09-22 21:06:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1100/1251]	eta 0:03:02 lr 0.000100	time 1.2048 (1.2062)	loss 253.2765 (260.6751)	attn_loss 247.2229 (254.4029)	hidden_loss 6.0536 (6.2722)	grad_norm 47.9546 (89.7397)	mem 26450MB
[2021-09-22 21:06:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1110/1251]	eta 0:02:50 lr 0.000100	time 1.1945 (1.2062)	loss 262.0721 (260.6983)	attn_loss 255.3269 (254.4261)	hidden_loss 6.7453 (6.2722)	grad_norm 124.9898 (89.7557)	mem 26450MB
[2021-09-22 21:06:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1120/1251]	eta 0:02:37 lr 0.000100	time 1.1878 (1.2061)	loss 262.9909 (260.6603)	attn_loss 256.7188 (254.3885)	hidden_loss 6.2721 (6.2719)	grad_norm 84.7916 (89.7829)	mem 26450MB
[2021-09-22 21:06:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1130/1251]	eta 0:02:25 lr 0.000100	time 1.2000 (1.2060)	loss 256.1351 (260.6622)	attn_loss 249.9860 (254.3903)	hidden_loss 6.1491 (6.2718)	grad_norm 66.9249 (89.6324)	mem 26450MB
[2021-09-22 21:06:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1140/1251]	eta 0:02:13 lr 0.000100	time 1.1936 (1.2060)	loss 267.2525 (260.6312)	attn_loss 261.0283 (254.3592)	hidden_loss 6.2242 (6.2720)	grad_norm 101.4078 (89.4763)	mem 26450MB
[2021-09-22 21:07:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1150/1251]	eta 0:02:01 lr 0.000100	time 1.1946 (1.2059)	loss 258.2965 (260.6048)	attn_loss 252.5790 (254.3336)	hidden_loss 5.7175 (6.2712)	grad_norm 76.8228 (89.3910)	mem 26450MB
[2021-09-22 21:07:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1160/1251]	eta 0:01:49 lr 0.000100	time 1.1936 (1.2059)	loss 265.9139 (260.6043)	attn_loss 259.6321 (254.3338)	hidden_loss 6.2817 (6.2706)	grad_norm 60.4231 (89.3420)	mem 26450MB
[2021-09-22 21:07:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1170/1251]	eta 0:01:37 lr 0.000100	time 1.1886 (1.2058)	loss 254.1838 (260.5913)	attn_loss 248.1389 (254.3212)	hidden_loss 6.0449 (6.2701)	grad_norm 137.7476 (89.3669)	mem 26450MB
[2021-09-22 21:07:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1180/1251]	eta 0:01:25 lr 0.000100	time 1.1969 (1.2057)	loss 257.7602 (260.5789)	attn_loss 251.6382 (254.3080)	hidden_loss 6.1220 (6.2710)	grad_norm 66.6414 (89.2676)	mem 26450MB
[2021-09-22 21:07:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1190/1251]	eta 0:01:13 lr 0.000100	time 1.2038 (1.2057)	loss 255.0182 (260.5564)	attn_loss 248.0782 (254.2856)	hidden_loss 6.9400 (6.2709)	grad_norm 52.7055 (89.0508)	mem 26450MB
[2021-09-22 21:08:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1200/1251]	eta 0:01:01 lr 0.000100	time 1.1884 (1.2057)	loss 264.1483 (260.5302)	attn_loss 257.8679 (254.2596)	hidden_loss 6.2804 (6.2706)	grad_norm 106.4618 (89.0045)	mem 26450MB
[2021-09-22 21:08:16 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1210/1251]	eta 0:00:49 lr 0.000100	time 1.1923 (1.2056)	loss 265.2062 (260.5243)	attn_loss 258.9650 (254.2544)	hidden_loss 6.2412 (6.2699)	grad_norm 90.1401 (89.0861)	mem 26450MB
[2021-09-22 21:08:28 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1220/1251]	eta 0:00:37 lr 0.000100	time 1.2129 (1.2055)	loss 255.6441 (260.5248)	attn_loss 249.4365 (254.2551)	hidden_loss 6.2077 (6.2697)	grad_norm 93.8630 (89.1814)	mem 26450MB
[2021-09-22 21:08:40 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1230/1251]	eta 0:00:25 lr 0.000100	time 1.1813 (1.2055)	loss 252.1615 (260.5091)	attn_loss 246.1007 (254.2397)	hidden_loss 6.0608 (6.2694)	grad_norm 80.2664 (89.0938)	mem 26450MB
[2021-09-22 21:08:52 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1240/1251]	eta 0:00:13 lr 0.000100	time 1.1895 (1.2054)	loss 258.6253 (260.4877)	attn_loss 252.6711 (254.2184)	hidden_loss 5.9542 (6.2692)	grad_norm 94.9592 (88.9085)	mem 26450MB
[2021-09-22 21:09:04 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [2/3][1250/1251]	eta 0:00:01 lr 0.000100	time 1.1936 (1.2053)	loss 255.2124 (260.4460)	attn_loss 249.0800 (254.1787)	hidden_loss 6.1323 (6.2673)	grad_norm 92.5072 (88.8115)	mem 26450MB
[2021-09-22 21:09:05 swin_tiny_patch4_window7_224] (main.py 371): INFO EPOCH 2 training takes 0:25:08
[2021-09-22 21:09:05 swin_tiny_patch4_window7_224] (utils.py 63): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth saving......
[2021-09-22 21:09:05 swin_tiny_patch4_window7_224] (utils.py 65): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth saved !!!
[2021-09-22 21:09:05 swin_tiny_patch4_window7_224] (main.py 233): INFO Training time 1:15:26
[2021-09-22 21:11:43 swin_tiny_patch4_window7_224] (main.py 720): INFO Full config saved to output/swin_tiny_patch4_window7_224/test_inter_all/config.json
[2021-09-22 21:11:43 swin_tiny_patch4_window7_224] (main.py 723): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /root/FastBaseline/data/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
DISTILL:
  ACCUMULATE_STEPS: 0
  ALPHA: 0.0
  DO_DISTILL: true
  INTERMEDIATE_CHECKPOINT: ''
  LOAD_TAR: false
  STAGE: -1
  TEACHER: /mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
  TEMPERATURE: 1.0
  TRAIN_INTERMEDIATE: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin_distill
OUTPUT: output/swin_tiny_patch4_window7_224/test_inter_all
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: test_inter_all
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EPOCHS: 3
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2021-09-22 21:11:48 swin_tiny_patch4_window7_224] (main.py 131): INFO Loading teacher model:swin_distill//mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 137): INFO <All keys matched successfully>
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 141): INFO Creating model:swin_distill/swin_tiny_patch4_window7_224
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 144): INFO SwinTransformerDistill(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayerDistill(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayerDistill(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayerDistill(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayerDistill(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
  (fit_dense_C): ModuleList(
    (0): Linear(in_features=96, out_features=192, bias=True)
    (1): Linear(in_features=192, out_features=384, bias=True)
    (2): Linear(in_features=384, out_features=768, bias=True)
    (3): Linear(in_features=768, out_features=1536, bias=True)
  )
  (fit_dense_M): ModuleList(
    (0): Linear(in_features=3, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=12, bias=True)
    (2): Linear(in_features=12, out_features=24, bias=True)
    (3): Linear(in_features=24, out_features=48, bias=True)
  )
)
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 154): INFO number of params: 29859574
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 157): INFO number of GFLOPs: 4.49440512
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (main.py 184): INFO auto resuming from output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth....................
[2021-09-22 21:11:53 swin_tiny_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-09-22 21:12:03 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [0/49]	Time 9.543 (9.543)	Attention_Loss_0 122.5377 (122.5377)	Attention_Loss_1 231.5536 (231.5536)	Attention_Loss_2 384.1242 (384.1242)	Attention_Loss_3 8.2346 (8.2346)	Hidden_Loss_0 0.5802 (0.5802)	Hidden_Loss_1 1.7199 (1.7199)	Hidden_Loss_2 1.7493 (1.7493)	Hidden_Loss_3 27.0803 (27.0803)	Mem 9282MB
[2021-09-22 21:12:19 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [10/49]	Time 1.615 (2.339)	Attention_Loss_0 122.9942 (122.5730)	Attention_Loss_1 235.8838 (231.9370)	Attention_Loss_2 384.0848 (383.7992)	Attention_Loss_3 8.2099 (8.2889)	Hidden_Loss_0 0.5778 (0.5785)	Hidden_Loss_1 1.6973 (1.7049)	Hidden_Loss_2 1.7247 (1.7338)	Hidden_Loss_3 28.3323 (27.6844)	Mem 9731MB
[2021-09-22 21:12:35 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [20/49]	Time 1.615 (1.992)	Attention_Loss_0 122.2601 (122.5663)	Attention_Loss_1 232.7396 (232.2036)	Attention_Loss_2 383.3393 (383.7696)	Attention_Loss_3 8.2755 (8.2988)	Hidden_Loss_0 0.5822 (0.5770)	Hidden_Loss_1 1.7140 (1.7114)	Hidden_Loss_2 1.7417 (1.7379)	Hidden_Loss_3 27.8904 (27.7320)	Mem 9731MB
[2021-09-22 21:12:51 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [30/49]	Time 1.617 (1.869)	Attention_Loss_0 121.7691 (122.4873)	Attention_Loss_1 231.7421 (231.8804)	Attention_Loss_2 382.8303 (383.6914)	Attention_Loss_3 8.3531 (8.3016)	Hidden_Loss_0 0.5763 (0.5764)	Hidden_Loss_1 1.7103 (1.7102)	Hidden_Loss_2 1.7531 (1.7387)	Hidden_Loss_3 28.0390 (27.7494)	Mem 9731MB
[2021-09-22 21:13:07 swin_tiny_patch4_window7_224] (main.py 591): INFO Test: [40/49]	Time 1.602 (1.804)	Attention_Loss_0 121.8845 (122.4926)	Attention_Loss_1 233.6212 (232.0097)	Attention_Loss_2 383.9327 (383.6747)	Attention_Loss_3 8.2955 (8.3048)	Hidden_Loss_0 0.5605 (0.5751)	Hidden_Loss_1 1.7188 (1.7113)	Hidden_Loss_2 1.7531 (1.7405)	Hidden_Loss_3 26.9176 (27.7171)	Mem 9731MB
[2021-09-22 21:13:20 swin_tiny_patch4_window7_224] (main.py 632): INFO  * Attention_Loss_0 122.472 Attention_Loss_1 232.119 Attention_Loss_2 383.713 Attention_Loss_3 8.303 Hidden_Loss_0 0.575 Hidden_Loss_1 1.711 Hidden_Loss_2 1.741 Hidden_Loss_3 27.743
[2021-09-22 21:13:20 swin_tiny_patch4_window7_224] (main.py 203): INFO Start training
[2021-09-22 21:13:20 swin_tiny_patch4_window7_224] (main.py 268): INFO Training stage: -1...
[2021-09-22 21:13:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][0/1251]	eta 3:06:49 lr 0.000100	time 8.9608 (8.9608)	loss 265.9203 (265.9203)	attn_loss 259.5306 (259.5306)	hidden_loss 6.3897 (6.3897)	grad_norm nan (nan)	mem 26097MB
[2021-09-22 21:13:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][10/1251]	eta 0:39:43 lr 0.000100	time 1.2011 (1.9208)	loss 260.7091 (271.2982)	attn_loss 253.4810 (264.8321)	hidden_loss 7.2281 (6.4662)	grad_norm 266.8941 (nan)	mem 26437MB
[2021-09-22 21:13:53 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][20/1251]	eta 0:32:20 lr 0.000100	time 1.1949 (1.5768)	loss 255.8652 (266.0308)	attn_loss 249.4088 (259.6391)	hidden_loss 6.4564 (6.3917)	grad_norm 67.4023 (nan)	mem 26443MB
[2021-09-22 21:14:05 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][30/1251]	eta 0:29:33 lr 0.000100	time 1.1959 (1.4525)	loss 261.2272 (264.1319)	attn_loss 254.9731 (257.7761)	hidden_loss 6.2541 (6.3558)	grad_norm 102.7612 (nan)	mem 26443MB
[2021-09-22 21:14:17 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][40/1251]	eta 0:28:03 lr 0.000100	time 1.1934 (1.3902)	loss 255.5547 (262.7021)	attn_loss 249.3431 (256.3977)	hidden_loss 6.2116 (6.3044)	grad_norm 113.0980 (nan)	mem 26443MB
[2021-09-22 21:14:29 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][50/1251]	eta 0:27:03 lr 0.000100	time 1.1987 (1.3515)	loss 265.6241 (262.0313)	attn_loss 259.4574 (255.7455)	hidden_loss 6.1668 (6.2859)	grad_norm 64.1882 (nan)	mem 26443MB
[2021-09-22 21:14:41 swin_tiny_patch4_window7_224] (main.py 361): INFO Train: [0/3][60/1251]	eta 0:26:19 lr 0.000100	time 1.2118 (1.3260)	loss 254.1567 (261.6683)	attn_loss 248.4740 (255.4023)	hidden_loss 5.6827 (6.2660)	grad_norm 73.9348 (nan)	mem 26443MB
[2021-09-22 21:16:17 swin_tiny_patch4_window7_224] (main.py 718): INFO Full config saved to output/swin_tiny_patch4_window7_224/test_inter_all/config.json
[2021-09-22 21:16:17 swin_tiny_patch4_window7_224] (main.py 721): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /root/FastBaseline/data/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
DISTILL:
  ACCUMULATE_STEPS: 0
  ALPHA: 0.0
  DO_DISTILL: true
  LOAD_TAR: false
  STAGE: -1
  TEACHER: /mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
  TEMPERATURE: 1.0
  TRAIN_INTERMEDIATE: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin_distill
OUTPUT: output/swin_tiny_patch4_window7_224/test_inter_all
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: test_inter_all
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EPOCHS: 1
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2021-09-22 21:16:23 swin_tiny_patch4_window7_224] (main.py 129): INFO Loading teacher model:swin_distill//mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 135): INFO <All keys matched successfully>
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 139): INFO Creating model:swin_distill/swin_tiny_patch4_window7_224
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 142): INFO SwinTransformerDistill(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayerDistill(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayerDistill(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayerDistill(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayerDistill(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
  (fit_dense_C): ModuleList(
    (0): Linear(in_features=96, out_features=192, bias=True)
    (1): Linear(in_features=192, out_features=384, bias=True)
    (2): Linear(in_features=384, out_features=768, bias=True)
    (3): Linear(in_features=768, out_features=1536, bias=True)
  )
  (fit_dense_M): ModuleList(
    (0): Linear(in_features=3, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=12, bias=True)
    (2): Linear(in_features=12, out_features=24, bias=True)
    (3): Linear(in_features=24, out_features=48, bias=True)
  )
)
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 152): INFO number of params: 29859574
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 155): INFO number of GFLOPs: 4.49440512
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (main.py 182): INFO auto resuming from output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth
[2021-09-22 21:16:28 swin_tiny_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth....................
[2021-09-22 21:16:29 swin_tiny_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-09-22 21:16:38 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [0/49]	Time 9.110 (9.110)	Attention_Loss_0 122.5377 (122.5377)	Attention_Loss_1 231.5536 (231.5536)	Attention_Loss_2 384.1242 (384.1242)	Attention_Loss_3 8.2346 (8.2346)	Hidden_Loss_0 0.5802 (0.5802)	Hidden_Loss_1 1.7199 (1.7199)	Hidden_Loss_2 1.7493 (1.7493)	Hidden_Loss_3 27.0803 (27.0803)	Mem 9282MB
[2021-09-22 21:18:25 swin_tiny_patch4_window7_224] (main.py 718): INFO Full config saved to output/swin_tiny_patch4_window7_224/test_inter_all/config.json
[2021-09-22 21:18:25 swin_tiny_patch4_window7_224] (main.py 721): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /root/FastBaseline/data/imagenet
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
DISTILL:
  ACCUMULATE_STEPS: 0
  ALPHA: 0.0
  DO_DISTILL: true
  LOAD_TAR: false
  STAGE: -1
  TEACHER: /mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
  TEMPERATURE: 1.0
  TRAIN_INTERMEDIATE: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin_distill
OUTPUT: output/swin_tiny_patch4_window7_224/test_inter_all
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: test_inter_all
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EPOCHS: 4
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 3
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2021-09-22 21:18:31 swin_tiny_patch4_window7_224] (main.py 129): INFO Loading teacher model:swin_distill//mnt/configblob/users/v-jinnian/swin_distill/trained_models/swin_large_patch4_window7_224_22kto1k.pth
[2021-09-22 21:18:35 swin_tiny_patch4_window7_224] (main.py 135): INFO <All keys matched successfully>
[2021-09-22 21:18:35 swin_tiny_patch4_window7_224] (main.py 139): INFO Creating model:swin_distill/swin_tiny_patch4_window7_224
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (main.py 142): INFO SwinTransformerDistill(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayerDistill(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayerDistill(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayerDistill(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlockDistill(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayerDistill(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlockDistill(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttentionDistill(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
  (fit_dense_C): ModuleList(
    (0): Linear(in_features=96, out_features=192, bias=True)
    (1): Linear(in_features=192, out_features=384, bias=True)
    (2): Linear(in_features=384, out_features=768, bias=True)
    (3): Linear(in_features=768, out_features=1536, bias=True)
  )
  (fit_dense_M): ModuleList(
    (0): Linear(in_features=3, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=12, bias=True)
    (2): Linear(in_features=12, out_features=24, bias=True)
    (3): Linear(in_features=24, out_features=48, bias=True)
  )
)
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (main.py 152): INFO number of params: 29859574
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (main.py 155): INFO number of GFLOPs: 4.49440512
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (main.py 182): INFO auto resuming from output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_2.pth....................
[2021-09-22 21:18:36 swin_tiny_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-09-22 21:18:45 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [0/49]	Time 8.736 (8.736)	Attention_Loss_0 122.5377 (122.5377)	Attention_Loss_1 231.5536 (231.5536)	Attention_Loss_2 384.1242 (384.1242)	Attention_Loss_3 8.2346 (8.2346)	Hidden_Loss_0 0.5802 (0.5802)	Hidden_Loss_1 1.7199 (1.7199)	Hidden_Loss_2 1.7493 (1.7493)	Hidden_Loss_3 27.0803 (27.0803)	Mem 9282MB
[2021-09-22 21:19:01 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [10/49]	Time 1.616 (2.271)	Attention_Loss_0 122.9942 (122.5730)	Attention_Loss_1 235.8838 (231.9370)	Attention_Loss_2 384.0848 (383.7992)	Attention_Loss_3 8.2099 (8.2889)	Hidden_Loss_0 0.5778 (0.5785)	Hidden_Loss_1 1.6973 (1.7049)	Hidden_Loss_2 1.7247 (1.7338)	Hidden_Loss_3 28.3323 (27.6844)	Mem 9731MB
[2021-09-22 21:19:17 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [20/49]	Time 1.601 (1.955)	Attention_Loss_0 122.2601 (122.5663)	Attention_Loss_1 232.7396 (232.2036)	Attention_Loss_2 383.3393 (383.7696)	Attention_Loss_3 8.2755 (8.2988)	Hidden_Loss_0 0.5822 (0.5770)	Hidden_Loss_1 1.7140 (1.7114)	Hidden_Loss_2 1.7417 (1.7379)	Hidden_Loss_3 27.8904 (27.7320)	Mem 9731MB
[2021-09-22 21:19:33 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [30/49]	Time 1.602 (1.843)	Attention_Loss_0 121.7691 (122.4873)	Attention_Loss_1 231.7421 (231.8804)	Attention_Loss_2 382.8303 (383.6914)	Attention_Loss_3 8.3531 (8.3016)	Hidden_Loss_0 0.5763 (0.5764)	Hidden_Loss_1 1.7103 (1.7102)	Hidden_Loss_2 1.7531 (1.7387)	Hidden_Loss_3 28.0390 (27.7494)	Mem 9731MB
[2021-09-22 21:19:49 swin_tiny_patch4_window7_224] (main.py 589): INFO Test: [40/49]	Time 1.603 (1.785)	Attention_Loss_0 121.8845 (122.4926)	Attention_Loss_1 233.6212 (232.0097)	Attention_Loss_2 383.9327 (383.6747)	Attention_Loss_3 8.2955 (8.3048)	Hidden_Loss_0 0.5605 (0.5751)	Hidden_Loss_1 1.7188 (1.7113)	Hidden_Loss_2 1.7531 (1.7405)	Hidden_Loss_3 26.9176 (27.7171)	Mem 9731MB
[2021-09-22 21:20:02 swin_tiny_patch4_window7_224] (main.py 630): INFO  * Attention_Loss_0 122.472 Attention_Loss_1 232.119 Attention_Loss_2 383.713 Attention_Loss_3 8.303 Hidden_Loss_0 0.575 Hidden_Loss_1 1.711 Hidden_Loss_2 1.741 Hidden_Loss_3 27.743
[2021-09-22 21:20:02 swin_tiny_patch4_window7_224] (main.py 201): INFO Start training
[2021-09-22 21:20:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][0/1251]	eta 3:02:05 lr 0.000100	time 8.7333 (8.7333)	loss 266.1450 (266.1450)	attn_loss 259.9578 (259.9578)	hidden_loss 6.1872 (6.1872)	grad_norm nan (nan)	mem 26097MB
[2021-09-22 21:20:23 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][10/1251]	eta 0:39:30 lr 0.000100	time 1.1921 (1.9099)	loss 267.9011 (272.6256)	attn_loss 260.3244 (266.0983)	hidden_loss 7.5767 (6.5273)	grad_norm 349.0736 (nan)	mem 26438MB
[2021-09-22 21:20:35 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][20/1251]	eta 0:32:13 lr 0.000100	time 1.2085 (1.5706)	loss 255.8445 (267.2669)	attn_loss 249.2371 (260.7947)	hidden_loss 6.6074 (6.4723)	grad_norm 128.9087 (nan)	mem 26439MB
[2021-09-22 21:20:47 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][30/1251]	eta 0:29:29 lr 0.000100	time 1.1897 (1.4495)	loss 262.0099 (265.1495)	attn_loss 255.6943 (258.7416)	hidden_loss 6.3155 (6.4079)	grad_norm 68.0227 (nan)	mem 26441MB
[2021-09-22 21:20:59 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][40/1251]	eta 0:28:01 lr 0.000100	time 1.1940 (1.3884)	loss 253.9465 (263.4966)	attn_loss 247.7410 (257.1595)	hidden_loss 6.2055 (6.3371)	grad_norm 58.9070 (nan)	mem 26442MB
[2021-09-22 21:21:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][50/1251]	eta 0:27:01 lr 0.000100	time 1.2094 (1.3505)	loss 267.0178 (262.7371)	attn_loss 260.6232 (256.4063)	hidden_loss 6.3946 (6.3308)	grad_norm 43.2871 (nan)	mem 26442MB
[2021-09-22 21:21:23 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][60/1251]	eta 0:26:19 lr 0.000100	time 1.2007 (1.3263)	loss 253.9904 (262.3186)	attn_loss 248.1940 (256.0031)	hidden_loss 5.7964 (6.3155)	grad_norm 50.7854 (nan)	mem 26442MB
[2021-09-22 21:21:35 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][70/1251]	eta 0:25:44 lr 0.000100	time 1.2074 (1.3082)	loss 251.7217 (261.8771)	attn_loss 245.1110 (255.5556)	hidden_loss 6.6107 (6.3215)	grad_norm 47.9678 (nan)	mem 26442MB
[2021-09-22 21:21:47 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][80/1251]	eta 0:25:15 lr 0.000100	time 1.2015 (1.2942)	loss 253.2444 (261.0828)	attn_loss 246.3705 (254.7915)	hidden_loss 6.8739 (6.2913)	grad_norm 79.3911 (nan)	mem 26442MB
[2021-09-22 21:21:59 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][90/1251]	eta 0:24:49 lr 0.000100	time 1.1870 (1.2830)	loss 266.7866 (260.8758)	attn_loss 260.6806 (254.6024)	hidden_loss 6.1060 (6.2733)	grad_norm 84.0823 (nan)	mem 26442MB
[2021-09-22 21:22:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][100/1251]	eta 0:24:27 lr 0.000100	time 1.2027 (1.2747)	loss 264.6676 (260.6744)	attn_loss 258.2483 (254.4112)	hidden_loss 6.4193 (6.2632)	grad_norm 76.2385 (nan)	mem 26442MB
[2021-09-22 21:22:23 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][110/1251]	eta 0:24:06 lr 0.000100	time 1.1915 (1.2676)	loss 251.4136 (260.5036)	attn_loss 245.4457 (254.2572)	hidden_loss 5.9679 (6.2464)	grad_norm 63.7386 (nan)	mem 26442MB
[2021-09-22 21:22:35 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][120/1251]	eta 0:23:47 lr 0.000100	time 1.2114 (1.2622)	loss 255.8822 (260.3835)	attn_loss 249.8167 (254.1374)	hidden_loss 6.0655 (6.2461)	grad_norm 67.0872 (nan)	mem 26444MB
[2021-09-22 21:22:47 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][130/1251]	eta 0:23:29 lr 0.000100	time 1.1859 (1.2571)	loss 256.1138 (260.2733)	attn_loss 249.9381 (254.0370)	hidden_loss 6.1757 (6.2363)	grad_norm 67.9815 (nan)	mem 26444MB
[2021-09-22 21:22:59 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][140/1251]	eta 0:23:11 lr 0.000100	time 1.2178 (1.2528)	loss 264.2698 (260.1573)	attn_loss 258.1846 (253.9262)	hidden_loss 6.0852 (6.2311)	grad_norm 71.5887 (nan)	mem 26444MB
[2021-09-22 21:23:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][150/1251]	eta 0:22:55 lr 0.000100	time 1.2075 (1.2490)	loss 255.1055 (260.0446)	attn_loss 248.5536 (253.8085)	hidden_loss 6.5519 (6.2361)	grad_norm 70.5148 (nan)	mem 26444MB
[2021-09-22 21:23:23 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][160/1251]	eta 0:22:39 lr 0.000100	time 1.1946 (1.2463)	loss 255.2832 (260.0763)	attn_loss 249.2242 (253.8387)	hidden_loss 6.0590 (6.2376)	grad_norm 82.8340 (nan)	mem 26444MB
[2021-09-22 21:23:35 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][170/1251]	eta 0:22:24 lr 0.000100	time 1.1940 (1.2434)	loss 254.1757 (259.9231)	attn_loss 247.6277 (253.6829)	hidden_loss 6.5480 (6.2402)	grad_norm 60.2410 (nan)	mem 26444MB
[2021-09-22 21:23:47 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][180/1251]	eta 0:22:09 lr 0.000100	time 1.1994 (1.2411)	loss 259.2383 (259.8719)	attn_loss 252.9141 (253.6373)	hidden_loss 6.3242 (6.2346)	grad_norm 103.6266 (nan)	mem 26444MB
[2021-09-22 21:23:59 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][190/1251]	eta 0:21:54 lr 0.000100	time 1.1847 (1.2388)	loss 253.0836 (259.8373)	attn_loss 247.3131 (253.6103)	hidden_loss 5.7705 (6.2270)	grad_norm 74.1599 (nan)	mem 26444MB
[2021-09-22 21:24:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][200/1251]	eta 0:21:39 lr 0.000100	time 1.1995 (1.2368)	loss 254.9512 (259.7119)	attn_loss 248.2438 (253.4888)	hidden_loss 6.7074 (6.2231)	grad_norm 65.1201 (nan)	mem 26444MB
[2021-09-22 21:24:23 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][210/1251]	eta 0:21:25 lr 0.000100	time 1.2101 (1.2351)	loss 258.1987 (259.6470)	attn_loss 252.0434 (253.4261)	hidden_loss 6.1553 (6.2209)	grad_norm 64.8384 (nan)	mem 26444MB
[2021-09-22 21:24:35 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][220/1251]	eta 0:21:11 lr 0.000100	time 1.1942 (1.2334)	loss 265.5988 (259.5933)	attn_loss 259.3439 (253.3744)	hidden_loss 6.2549 (6.2189)	grad_norm 93.3581 (nan)	mem 26444MB
[2021-09-22 21:24:47 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][230/1251]	eta 0:20:57 lr 0.000100	time 1.1910 (1.2318)	loss 254.2624 (259.5947)	attn_loss 248.3005 (253.3796)	hidden_loss 5.9619 (6.2151)	grad_norm 88.2315 (nan)	mem 26444MB
[2021-09-22 21:24:59 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][240/1251]	eta 0:20:44 lr 0.000100	time 1.1886 (1.2305)	loss 262.9747 (259.6229)	attn_loss 256.7235 (253.4067)	hidden_loss 6.2512 (6.2162)	grad_norm 90.2702 (nan)	mem 26444MB
[2021-09-22 21:25:11 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][250/1251]	eta 0:20:30 lr 0.000100	time 1.2041 (1.2292)	loss 264.7536 (259.5480)	attn_loss 258.4800 (253.3293)	hidden_loss 6.2736 (6.2188)	grad_norm 80.8199 (nan)	mem 26444MB
[2021-09-22 21:25:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][260/1251]	eta 0:20:16 lr 0.000100	time 1.2080 (1.2279)	loss 265.1352 (259.4432)	attn_loss 258.7274 (253.2303)	hidden_loss 6.4077 (6.2129)	grad_norm 102.9286 (nan)	mem 26444MB
[2021-09-22 21:25:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][270/1251]	eta 0:20:03 lr 0.000100	time 1.1866 (1.2268)	loss 265.7595 (259.5051)	attn_loss 259.3578 (253.2901)	hidden_loss 6.4016 (6.2151)	grad_norm 90.1401 (nan)	mem 26444MB
[2021-09-22 21:25:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][280/1251]	eta 0:19:50 lr 0.000100	time 1.1996 (1.2257)	loss 253.2599 (259.3561)	attn_loss 246.9159 (253.1437)	hidden_loss 6.3440 (6.2124)	grad_norm 104.5117 (nan)	mem 26444MB
[2021-09-22 21:25:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][290/1251]	eta 0:19:36 lr 0.000100	time 1.1928 (1.2246)	loss 251.5706 (259.3303)	attn_loss 245.4173 (253.1143)	hidden_loss 6.1534 (6.2160)	grad_norm 94.1905 (nan)	mem 26444MB
[2021-09-22 21:26:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][300/1251]	eta 0:19:23 lr 0.000100	time 1.1866 (1.2237)	loss 254.3370 (259.2455)	attn_loss 248.5530 (253.0272)	hidden_loss 5.7840 (6.2183)	grad_norm 94.9614 (nan)	mem 26444MB
[2021-09-22 21:26:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][310/1251]	eta 0:19:10 lr 0.000100	time 1.1858 (1.2228)	loss 252.5997 (259.1678)	attn_loss 246.4728 (252.9484)	hidden_loss 6.1269 (6.2193)	grad_norm 114.5429 (nan)	mem 26444MB
[2021-09-22 21:26:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][320/1251]	eta 0:18:57 lr 0.000100	time 1.1969 (1.2221)	loss 263.8343 (259.1395)	attn_loss 257.6425 (252.9233)	hidden_loss 6.1919 (6.2163)	grad_norm 87.6239 (nan)	mem 26444MB
[2021-09-22 21:26:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][330/1251]	eta 0:18:44 lr 0.000100	time 1.2028 (1.2213)	loss 254.2844 (259.1292)	attn_loss 248.1060 (252.9141)	hidden_loss 6.1784 (6.2150)	grad_norm 73.9713 (nan)	mem 26444MB
[2021-09-22 21:26:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][340/1251]	eta 0:18:32 lr 0.000100	time 1.2038 (1.2207)	loss 253.7439 (259.0217)	attn_loss 247.8350 (252.8080)	hidden_loss 5.9089 (6.2137)	grad_norm 80.9570 (nan)	mem 26444MB
[2021-09-22 21:27:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][350/1251]	eta 0:18:19 lr 0.000100	time 1.2254 (1.2201)	loss 265.5653 (259.0442)	attn_loss 259.0373 (252.8329)	hidden_loss 6.5280 (6.2113)	grad_norm 56.1491 (nan)	mem 26444MB
[2021-09-22 21:27:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][360/1251]	eta 0:18:06 lr 0.000100	time 1.1963 (1.2196)	loss 261.6630 (259.0396)	attn_loss 255.4313 (252.8280)	hidden_loss 6.2317 (6.2116)	grad_norm 63.7264 (nan)	mem 26444MB
[2021-09-22 21:27:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][370/1251]	eta 0:17:53 lr 0.000100	time 1.2102 (1.2190)	loss 251.0779 (259.0019)	attn_loss 244.6255 (252.7905)	hidden_loss 6.4524 (6.2114)	grad_norm 134.2852 (nan)	mem 26444MB
[2021-09-22 21:27:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][380/1251]	eta 0:17:41 lr 0.000100	time 1.1869 (1.2186)	loss 253.1505 (258.9971)	attn_loss 246.9394 (252.7864)	hidden_loss 6.2111 (6.2107)	grad_norm 75.0710 (nan)	mem 26444MB
[2021-09-22 21:27:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][390/1251]	eta 0:17:28 lr 0.000100	time 1.2054 (1.2182)	loss 266.8968 (259.0168)	attn_loss 260.5605 (252.8065)	hidden_loss 6.3364 (6.2103)	grad_norm 78.9677 (nan)	mem 26444MB
[2021-09-22 21:28:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][400/1251]	eta 0:17:16 lr 0.000100	time 1.1931 (1.2176)	loss 250.8486 (259.0157)	attn_loss 245.0299 (252.8086)	hidden_loss 5.8187 (6.2070)	grad_norm 58.8054 (nan)	mem 26444MB
[2021-09-22 21:28:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][410/1251]	eta 0:17:03 lr 0.000100	time 1.1959 (1.2172)	loss 257.1211 (259.0205)	attn_loss 251.0437 (252.8140)	hidden_loss 6.0774 (6.2065)	grad_norm 72.8320 (nan)	mem 26444MB
[2021-09-22 21:28:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][420/1251]	eta 0:16:51 lr 0.000100	time 1.1929 (1.2167)	loss 263.6509 (259.0136)	attn_loss 257.4546 (252.8075)	hidden_loss 6.1963 (6.2060)	grad_norm 65.7054 (nan)	mem 26444MB
[2021-09-22 21:28:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][430/1251]	eta 0:16:38 lr 0.000100	time 1.1933 (1.2164)	loss 263.9995 (259.0399)	attn_loss 257.6360 (252.8366)	hidden_loss 6.3636 (6.2033)	grad_norm 76.1457 (nan)	mem 26444MB
[2021-09-22 21:28:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][440/1251]	eta 0:16:26 lr 0.000100	time 1.1991 (1.2159)	loss 265.2030 (259.0318)	attn_loss 259.0014 (252.8303)	hidden_loss 6.2017 (6.2015)	grad_norm 89.5720 (nan)	mem 26444MB
[2021-09-22 21:29:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][450/1251]	eta 0:16:13 lr 0.000100	time 1.2007 (1.2154)	loss 262.7900 (259.0334)	attn_loss 256.6193 (252.8328)	hidden_loss 6.1707 (6.2005)	grad_norm 70.2500 (nan)	mem 26445MB
[2021-09-22 21:29:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][460/1251]	eta 0:16:01 lr 0.000100	time 1.1968 (1.2150)	loss 253.8482 (259.0034)	attn_loss 247.8648 (252.8040)	hidden_loss 5.9834 (6.1995)	grad_norm 48.5733 (nan)	mem 26445MB
[2021-09-22 21:29:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][470/1251]	eta 0:15:48 lr 0.000100	time 1.1953 (1.2147)	loss 252.6675 (258.9355)	attn_loss 246.6488 (252.7366)	hidden_loss 6.0188 (6.1989)	grad_norm 84.1209 (nan)	mem 26445MB
[2021-09-22 21:29:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][480/1251]	eta 0:15:36 lr 0.000100	time 1.2107 (1.2144)	loss 265.0839 (258.9221)	attn_loss 258.9373 (252.7214)	hidden_loss 6.1466 (6.2007)	grad_norm 101.6172 (nan)	mem 26445MB
[2021-09-22 21:29:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][490/1251]	eta 0:15:23 lr 0.000100	time 1.2136 (1.2142)	loss 256.6652 (258.9373)	attn_loss 250.6517 (252.7349)	hidden_loss 6.0135 (6.2024)	grad_norm 89.4080 (nan)	mem 26445MB
[2021-09-22 21:30:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][500/1251]	eta 0:15:11 lr 0.000100	time 1.2215 (1.2140)	loss 252.0676 (258.9440)	attn_loss 246.0065 (252.7409)	hidden_loss 6.0611 (6.2031)	grad_norm 57.8652 (nan)	mem 26445MB
[2021-09-22 21:30:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][510/1251]	eta 0:14:59 lr 0.000100	time 1.2088 (1.2137)	loss 262.0439 (258.9129)	attn_loss 255.9430 (252.7123)	hidden_loss 6.1008 (6.2006)	grad_norm 59.0456 (nan)	mem 26445MB
[2021-09-22 21:30:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][520/1251]	eta 0:14:47 lr 0.000100	time 1.2067 (1.2134)	loss 260.3114 (258.8873)	attn_loss 254.1115 (252.6892)	hidden_loss 6.2000 (6.1981)	grad_norm 82.5927 (nan)	mem 26445MB
[2021-09-22 21:30:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][530/1251]	eta 0:14:34 lr 0.000100	time 1.2100 (1.2132)	loss 253.9710 (258.8130)	attn_loss 247.8097 (252.6175)	hidden_loss 6.1613 (6.1955)	grad_norm 73.3171 (nan)	mem 26445MB
[2021-09-22 21:30:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][540/1251]	eta 0:14:22 lr 0.000100	time 1.2013 (1.2129)	loss 258.2645 (258.8038)	attn_loss 251.5630 (252.6088)	hidden_loss 6.7015 (6.1951)	grad_norm 84.0896 (nan)	mem 26445MB
[2021-09-22 21:31:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][550/1251]	eta 0:14:10 lr 0.000100	time 1.2028 (1.2127)	loss 266.4892 (258.8485)	attn_loss 260.1620 (252.6533)	hidden_loss 6.3271 (6.1952)	grad_norm 84.5767 (nan)	mem 26445MB
[2021-09-22 21:31:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][560/1251]	eta 0:13:57 lr 0.000100	time 1.1935 (1.2125)	loss 250.9052 (258.8233)	attn_loss 244.5873 (252.6292)	hidden_loss 6.3179 (6.1941)	grad_norm 78.0311 (nan)	mem 26445MB
[2021-09-22 21:31:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][570/1251]	eta 0:13:45 lr 0.000100	time 1.2089 (1.2122)	loss 256.0042 (258.7726)	attn_loss 250.2781 (252.5806)	hidden_loss 5.7262 (6.1920)	grad_norm 73.3879 (nan)	mem 26445MB
[2021-09-22 21:31:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][580/1251]	eta 0:13:33 lr 0.000100	time 1.1814 (1.2120)	loss 264.5912 (258.7915)	attn_loss 258.4417 (252.6005)	hidden_loss 6.1495 (6.1910)	grad_norm 72.2118 (nan)	mem 26445MB
[2021-09-22 21:31:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][590/1251]	eta 0:13:20 lr 0.000100	time 1.1938 (1.2117)	loss 256.0777 (258.7942)	attn_loss 249.9093 (252.6024)	hidden_loss 6.1684 (6.1918)	grad_norm 70.9994 (nan)	mem 26445MB
[2021-09-22 21:32:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][600/1251]	eta 0:13:08 lr 0.000100	time 1.1996 (1.2116)	loss 249.4193 (258.7669)	attn_loss 243.6381 (252.5757)	hidden_loss 5.7811 (6.1912)	grad_norm 90.6321 (nan)	mem 26445MB
[2021-09-22 21:32:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][610/1251]	eta 0:12:56 lr 0.000100	time 1.1982 (1.2114)	loss 269.9147 (258.7885)	attn_loss 263.6392 (252.5996)	hidden_loss 6.2755 (6.1889)	grad_norm 56.0900 (nan)	mem 26445MB
[2021-09-22 21:32:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][620/1251]	eta 0:12:44 lr 0.000100	time 1.1918 (1.2113)	loss 264.0434 (258.7811)	attn_loss 257.8021 (252.5915)	hidden_loss 6.2413 (6.1896)	grad_norm 57.5302 (nan)	mem 26445MB
[2021-09-22 21:32:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][630/1251]	eta 0:12:32 lr 0.000100	time 1.1919 (1.2111)	loss 252.5219 (258.7782)	attn_loss 246.4094 (252.5886)	hidden_loss 6.1124 (6.1896)	grad_norm 49.0041 (nan)	mem 26445MB
[2021-09-22 21:32:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][640/1251]	eta 0:12:19 lr 0.000100	time 1.2031 (1.2109)	loss 253.8854 (258.7948)	attn_loss 247.5397 (252.6046)	hidden_loss 6.3457 (6.1902)	grad_norm 90.3877 (nan)	mem 26445MB
[2021-09-22 21:33:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][650/1251]	eta 0:12:07 lr 0.000100	time 1.1868 (1.2107)	loss 264.5902 (258.8006)	attn_loss 258.6217 (252.6097)	hidden_loss 5.9686 (6.1909)	grad_norm 59.5394 (nan)	mem 26445MB
[2021-09-22 21:33:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][660/1251]	eta 0:11:55 lr 0.000100	time 1.2083 (1.2105)	loss 252.7388 (258.7748)	attn_loss 246.6400 (252.5848)	hidden_loss 6.0988 (6.1900)	grad_norm 71.5157 (nan)	mem 26445MB
[2021-09-22 21:33:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][670/1251]	eta 0:11:43 lr 0.000100	time 1.2070 (1.2104)	loss 264.9866 (258.7950)	attn_loss 258.9845 (252.6062)	hidden_loss 6.0021 (6.1888)	grad_norm 66.5250 (nan)	mem 26445MB
[2021-09-22 21:33:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][680/1251]	eta 0:11:31 lr 0.000100	time 1.1918 (1.2102)	loss 263.6080 (258.8268)	attn_loss 257.5342 (252.6380)	hidden_loss 6.0738 (6.1888)	grad_norm 55.0199 (nan)	mem 26445MB
[2021-09-22 21:33:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][690/1251]	eta 0:11:18 lr 0.000100	time 1.2024 (1.2100)	loss 253.9893 (258.8238)	attn_loss 247.2362 (252.6333)	hidden_loss 6.7531 (6.1905)	grad_norm 58.7152 (nan)	mem 26445MB
[2021-09-22 21:34:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][700/1251]	eta 0:11:06 lr 0.000100	time 1.1992 (1.2099)	loss 254.8668 (258.7749)	attn_loss 248.5142 (252.5854)	hidden_loss 6.3526 (6.1895)	grad_norm 61.6901 (nan)	mem 26446MB
[2021-09-22 21:34:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][710/1251]	eta 0:10:54 lr 0.000100	time 1.2050 (1.2098)	loss 255.0275 (258.7554)	attn_loss 248.7911 (252.5659)	hidden_loss 6.2364 (6.1895)	grad_norm 64.1738 (nan)	mem 26446MB
[2021-09-22 21:34:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][720/1251]	eta 0:10:42 lr 0.000100	time 1.2153 (1.2097)	loss 263.4984 (258.7649)	attn_loss 257.2908 (252.5743)	hidden_loss 6.2075 (6.1905)	grad_norm 80.6726 (nan)	mem 26446MB
[2021-09-22 21:34:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][730/1251]	eta 0:10:30 lr 0.000100	time 1.2050 (1.2096)	loss 252.8499 (258.7896)	attn_loss 246.6047 (252.5986)	hidden_loss 6.2452 (6.1911)	grad_norm 51.4161 (nan)	mem 26446MB
[2021-09-22 21:34:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][740/1251]	eta 0:10:17 lr 0.000100	time 1.1820 (1.2094)	loss 255.6374 (258.7700)	attn_loss 249.5532 (252.5805)	hidden_loss 6.0842 (6.1895)	grad_norm 77.5364 (nan)	mem 26446MB
[2021-09-22 21:35:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][750/1251]	eta 0:10:05 lr 0.000100	time 1.1850 (1.2093)	loss 253.8204 (258.7263)	attn_loss 247.8535 (252.5369)	hidden_loss 5.9668 (6.1894)	grad_norm 76.7701 (nan)	mem 26446MB
[2021-09-22 21:35:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][760/1251]	eta 0:09:53 lr 0.000100	time 1.1802 (1.2091)	loss 251.2713 (258.6727)	attn_loss 245.1888 (252.4857)	hidden_loss 6.0825 (6.1870)	grad_norm 72.4857 (nan)	mem 26446MB
[2021-09-22 21:35:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][770/1251]	eta 0:09:41 lr 0.000100	time 1.2080 (1.2089)	loss 264.6989 (258.6826)	attn_loss 258.3724 (252.4948)	hidden_loss 6.3266 (6.1878)	grad_norm 54.4800 (nan)	mem 26446MB
[2021-09-22 21:35:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][780/1251]	eta 0:09:29 lr 0.000100	time 1.1995 (1.2089)	loss 263.7710 (258.6710)	attn_loss 257.7968 (252.4840)	hidden_loss 5.9742 (6.1870)	grad_norm 48.6464 (nan)	mem 26446MB
[2021-09-22 21:35:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][790/1251]	eta 0:09:17 lr 0.000100	time 1.2452 (1.2088)	loss 252.9613 (258.6554)	attn_loss 246.3639 (252.4669)	hidden_loss 6.5973 (6.1885)	grad_norm 76.4209 (nan)	mem 26446MB
[2021-09-22 21:36:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][800/1251]	eta 0:09:05 lr 0.000100	time 1.2043 (1.2087)	loss 264.9171 (258.6695)	attn_loss 258.7518 (252.4820)	hidden_loss 6.1653 (6.1875)	grad_norm 100.9517 (nan)	mem 26446MB
[2021-09-22 21:36:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][810/1251]	eta 0:08:52 lr 0.000100	time 1.2224 (1.2085)	loss 256.2750 (258.6259)	attn_loss 250.0966 (252.4396)	hidden_loss 6.1784 (6.1863)	grad_norm 95.0486 (nan)	mem 26446MB
[2021-09-22 21:36:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][820/1251]	eta 0:08:40 lr 0.000100	time 1.1916 (1.2084)	loss 265.7413 (258.6204)	attn_loss 259.3566 (252.4340)	hidden_loss 6.3847 (6.1864)	grad_norm 86.5909 (nan)	mem 26446MB
[2021-09-22 21:36:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][830/1251]	eta 0:08:28 lr 0.000100	time 1.1921 (1.2083)	loss 250.1548 (258.5781)	attn_loss 244.3519 (252.3923)	hidden_loss 5.8030 (6.1859)	grad_norm 64.9760 (nan)	mem 26446MB
[2021-09-22 21:36:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][840/1251]	eta 0:08:16 lr 0.000100	time 1.1909 (1.2082)	loss 262.6436 (258.6003)	attn_loss 256.6484 (252.4148)	hidden_loss 5.9952 (6.1855)	grad_norm 50.2974 (nan)	mem 26446MB
[2021-09-22 21:37:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][850/1251]	eta 0:08:04 lr 0.000100	time 1.2164 (1.2082)	loss 263.8799 (258.5896)	attn_loss 257.7222 (252.4046)	hidden_loss 6.1578 (6.1851)	grad_norm 76.0751 (nan)	mem 26446MB
[2021-09-22 21:37:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][860/1251]	eta 0:07:52 lr 0.000100	time 1.1893 (1.2080)	loss 259.7799 (258.6020)	attn_loss 253.6623 (252.4171)	hidden_loss 6.1176 (6.1849)	grad_norm 91.4382 (nan)	mem 26446MB
[2021-09-22 21:37:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][870/1251]	eta 0:07:40 lr 0.000100	time 1.1921 (1.2079)	loss 263.8540 (258.5785)	attn_loss 257.6125 (252.3944)	hidden_loss 6.2415 (6.1842)	grad_norm 80.8937 (nan)	mem 26446MB
[2021-09-22 21:37:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][880/1251]	eta 0:07:28 lr 0.000100	time 1.2047 (1.2079)	loss 266.1516 (258.5622)	attn_loss 259.9232 (252.3778)	hidden_loss 6.2284 (6.1844)	grad_norm 63.0955 (nan)	mem 26446MB
[2021-09-22 21:37:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][890/1251]	eta 0:07:16 lr 0.000100	time 1.1873 (1.2078)	loss 252.0701 (258.5539)	attn_loss 246.0583 (252.3707)	hidden_loss 6.0118 (6.1832)	grad_norm 145.2738 (nan)	mem 26446MB
[2021-09-22 21:38:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][900/1251]	eta 0:07:03 lr 0.000100	time 1.1877 (1.2076)	loss 259.4971 (258.5467)	attn_loss 253.3485 (252.3634)	hidden_loss 6.1487 (6.1832)	grad_norm 86.1304 (nan)	mem 26446MB
[2021-09-22 21:38:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][910/1251]	eta 0:06:51 lr 0.000100	time 1.1936 (1.2076)	loss 252.4971 (258.5401)	attn_loss 246.6576 (252.3575)	hidden_loss 5.8394 (6.1826)	grad_norm 58.3189 (nan)	mem 26446MB
[2021-09-22 21:38:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][920/1251]	eta 0:06:39 lr 0.000100	time 1.1953 (1.2075)	loss 265.5897 (258.5484)	attn_loss 259.4517 (252.3660)	hidden_loss 6.1380 (6.1824)	grad_norm 52.6331 (nan)	mem 26446MB
[2021-09-22 21:38:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][930/1251]	eta 0:06:27 lr 0.000100	time 1.2136 (1.2074)	loss 255.2663 (258.5452)	attn_loss 249.1412 (252.3636)	hidden_loss 6.1251 (6.1816)	grad_norm 69.0335 (nan)	mem 26446MB
[2021-09-22 21:38:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][940/1251]	eta 0:06:15 lr 0.000100	time 1.1983 (1.2073)	loss 267.2083 (258.5476)	attn_loss 260.9598 (252.3663)	hidden_loss 6.2485 (6.1813)	grad_norm 65.7185 (nan)	mem 26446MB
[2021-09-22 21:39:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][950/1251]	eta 0:06:03 lr 0.000100	time 1.1962 (1.2073)	loss 251.9764 (258.5100)	attn_loss 245.8153 (252.3290)	hidden_loss 6.1611 (6.1810)	grad_norm 78.6046 (nan)	mem 26446MB
[2021-09-22 21:39:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][960/1251]	eta 0:05:51 lr 0.000100	time 1.1977 (1.2072)	loss 254.8615 (258.5122)	attn_loss 248.7690 (252.3325)	hidden_loss 6.0925 (6.1796)	grad_norm 45.4092 (nan)	mem 26446MB
[2021-09-22 21:39:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][970/1251]	eta 0:05:39 lr 0.000100	time 1.1931 (1.2071)	loss 251.4235 (258.4683)	attn_loss 245.0323 (252.2903)	hidden_loss 6.3912 (6.1780)	grad_norm 91.2982 (nan)	mem 26446MB
[2021-09-22 21:39:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][980/1251]	eta 0:05:27 lr 0.000100	time 1.2076 (1.2070)	loss 255.7537 (258.4575)	attn_loss 250.0301 (252.2803)	hidden_loss 5.7236 (6.1772)	grad_norm 72.6122 (nan)	mem 26446MB
[2021-09-22 21:39:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][990/1251]	eta 0:05:15 lr 0.000100	time 1.2064 (1.2069)	loss 250.7290 (258.4173)	attn_loss 244.6204 (252.2416)	hidden_loss 6.1087 (6.1757)	grad_norm 78.3138 (nan)	mem 26446MB
[2021-09-22 21:40:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1000/1251]	eta 0:05:02 lr 0.000100	time 1.2183 (1.2069)	loss 261.4235 (258.3987)	attn_loss 254.7307 (252.2218)	hidden_loss 6.6928 (6.1769)	grad_norm 105.3891 (nan)	mem 26446MB
[2021-09-22 21:40:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1010/1251]	eta 0:04:50 lr 0.000100	time 1.1922 (1.2069)	loss 251.8757 (258.3885)	attn_loss 245.5172 (252.2125)	hidden_loss 6.3585 (6.1759)	grad_norm 94.0097 (nan)	mem 26446MB
[2021-09-22 21:40:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1020/1251]	eta 0:04:38 lr 0.000100	time 1.2066 (1.2068)	loss 265.7969 (258.4088)	attn_loss 259.4636 (252.2329)	hidden_loss 6.3333 (6.1759)	grad_norm 73.1341 (nan)	mem 26446MB
[2021-09-22 21:40:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1030/1251]	eta 0:04:26 lr 0.000100	time 1.1990 (1.2068)	loss 253.6684 (258.3634)	attn_loss 247.4791 (252.1876)	hidden_loss 6.1893 (6.1758)	grad_norm 57.1388 (nan)	mem 26446MB
[2021-09-22 21:40:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1040/1251]	eta 0:04:14 lr 0.000100	time 1.1939 (1.2067)	loss 264.8908 (258.3914)	attn_loss 258.7135 (252.2160)	hidden_loss 6.1773 (6.1755)	grad_norm 71.7574 (nan)	mem 26446MB
[2021-09-22 21:41:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1050/1251]	eta 0:04:02 lr 0.000100	time 1.1894 (1.2066)	loss 251.4055 (258.3773)	attn_loss 245.0620 (252.2020)	hidden_loss 6.3435 (6.1754)	grad_norm 62.7388 (nan)	mem 26446MB
[2021-09-22 21:41:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1060/1251]	eta 0:03:50 lr 0.000100	time 1.1853 (1.2066)	loss 259.4857 (258.3664)	attn_loss 253.1382 (252.1910)	hidden_loss 6.3475 (6.1754)	grad_norm 54.7952 (nan)	mem 26446MB
[2021-09-22 21:41:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1070/1251]	eta 0:03:38 lr 0.000100	time 1.1859 (1.2065)	loss 254.0041 (258.3726)	attn_loss 247.9391 (252.1964)	hidden_loss 6.0651 (6.1762)	grad_norm 90.5954 (nan)	mem 26446MB
[2021-09-22 21:41:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1080/1251]	eta 0:03:26 lr 0.000100	time 1.1946 (1.2065)	loss 251.6584 (258.3368)	attn_loss 245.6192 (252.1603)	hidden_loss 6.0391 (6.1765)	grad_norm 61.8302 (nan)	mem 26446MB
[2021-09-22 21:41:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1090/1251]	eta 0:03:14 lr 0.000100	time 1.1980 (1.2064)	loss 252.9692 (258.3290)	attn_loss 246.8612 (252.1540)	hidden_loss 6.1080 (6.1750)	grad_norm 76.4067 (nan)	mem 26446MB
[2021-09-22 21:42:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1100/1251]	eta 0:03:02 lr 0.000100	time 1.1935 (1.2063)	loss 253.8583 (258.3262)	attn_loss 247.9183 (252.1508)	hidden_loss 5.9399 (6.1753)	grad_norm 59.0006 (nan)	mem 26446MB
[2021-09-22 21:42:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1110/1251]	eta 0:02:50 lr 0.000100	time 1.1801 (1.2062)	loss 265.9535 (258.3344)	attn_loss 259.5983 (252.1594)	hidden_loss 6.3552 (6.1750)	grad_norm 94.3335 (nan)	mem 26446MB
[2021-09-22 21:42:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1120/1251]	eta 0:02:38 lr 0.000100	time 1.2119 (1.2063)	loss 249.9173 (258.3061)	attn_loss 244.0686 (252.1318)	hidden_loss 5.8487 (6.1743)	grad_norm 74.7767 (nan)	mem 26446MB
[2021-09-22 21:42:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1130/1251]	eta 0:02:25 lr 0.000100	time 1.1814 (1.2062)	loss 262.1355 (258.2992)	attn_loss 256.1058 (252.1254)	hidden_loss 6.0297 (6.1738)	grad_norm 75.5685 (nan)	mem 26446MB
[2021-09-22 21:42:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1140/1251]	eta 0:02:13 lr 0.000100	time 1.2333 (1.2062)	loss 248.6257 (258.2651)	attn_loss 243.0675 (252.0921)	hidden_loss 5.5583 (6.1730)	grad_norm 109.5280 (nan)	mem 26446MB
[2021-09-22 21:43:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1150/1251]	eta 0:02:01 lr 0.000100	time 1.2049 (1.2061)	loss 252.8747 (258.2530)	attn_loss 246.8922 (252.0793)	hidden_loss 5.9825 (6.1737)	grad_norm 95.2995 (nan)	mem 26446MB
[2021-09-22 21:43:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1160/1251]	eta 0:01:49 lr 0.000100	time 1.1873 (1.2061)	loss 264.6386 (258.2505)	attn_loss 258.2770 (252.0771)	hidden_loss 6.3616 (6.1734)	grad_norm 69.9887 (nan)	mem 26446MB
[2021-09-22 21:43:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1170/1251]	eta 0:01:37 lr 0.000100	time 1.1946 (1.2060)	loss 250.7064 (258.2379)	attn_loss 244.4571 (252.0645)	hidden_loss 6.2493 (6.1734)	grad_norm 67.6417 (nan)	mem 26446MB
[2021-09-22 21:43:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1180/1251]	eta 0:01:25 lr 0.000100	time 1.1868 (1.2060)	loss 261.5750 (258.2548)	attn_loss 254.7835 (252.0810)	hidden_loss 6.7914 (6.1738)	grad_norm 60.7154 (nan)	mem 26446MB
[2021-09-22 21:43:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1190/1251]	eta 0:01:13 lr 0.000100	time 1.1908 (1.2060)	loss 249.2614 (258.2436)	attn_loss 243.3589 (252.0707)	hidden_loss 5.9026 (6.1729)	grad_norm 60.5042 (nan)	mem 26446MB
[2021-09-22 21:44:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1200/1251]	eta 0:01:01 lr 0.000100	time 1.1822 (1.2059)	loss 264.1075 (258.2564)	attn_loss 257.9248 (252.0836)	hidden_loss 6.1827 (6.1728)	grad_norm 52.1321 (nan)	mem 26446MB
[2021-09-22 21:44:22 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1210/1251]	eta 0:00:49 lr 0.000100	time 1.2224 (1.2059)	loss 259.0205 (258.2593)	attn_loss 252.8151 (252.0874)	hidden_loss 6.2054 (6.1719)	grad_norm 75.1734 (nan)	mem 26446MB
[2021-09-22 21:44:34 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1220/1251]	eta 0:00:37 lr 0.000100	time 1.1873 (1.2058)	loss 254.9900 (258.2591)	attn_loss 249.1297 (252.0873)	hidden_loss 5.8603 (6.1718)	grad_norm 78.0953 (nan)	mem 26446MB
[2021-09-22 21:44:46 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1230/1251]	eta 0:00:25 lr 0.000100	time 1.1866 (1.2057)	loss 259.1383 (258.2329)	attn_loss 252.9291 (252.0613)	hidden_loss 6.2092 (6.1716)	grad_norm 81.1385 (nan)	mem 26446MB
[2021-09-22 21:44:58 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1240/1251]	eta 0:00:13 lr 0.000100	time 1.1893 (1.2057)	loss 265.1482 (258.2349)	attn_loss 259.0550 (252.0637)	hidden_loss 6.0932 (6.1712)	grad_norm 61.7142 (nan)	mem 26446MB
[2021-09-22 21:45:10 swin_tiny_patch4_window7_224] (main.py 359): INFO Train: [3/4][1250/1251]	eta 0:00:01 lr 0.000100	time 1.1954 (1.2056)	loss 263.1931 (258.2067)	attn_loss 257.1522 (252.0351)	hidden_loss 6.0409 (6.1717)	grad_norm 85.1494 (nan)	mem 26446MB
[2021-09-22 21:45:11 swin_tiny_patch4_window7_224] (main.py 369): INFO EPOCH 3 training takes 0:25:08
[2021-09-22 21:45:11 swin_tiny_patch4_window7_224] (utils.py 63): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_3.pth saving......
[2021-09-22 21:45:11 swin_tiny_patch4_window7_224] (utils.py 65): INFO output/swin_tiny_patch4_window7_224/test_inter_all/ckpt_epoch_3.pth saved !!!
[2021-09-22 21:45:11 swin_tiny_patch4_window7_224] (main.py 231): INFO Training time 0:25:09
